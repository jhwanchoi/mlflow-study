# Phase 3: í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê³ ë„í™” ìƒì„¸ ê³„íš

**ìž‘ì„±ì¼**: 2025-10-18
**ë²„ì „**: 1.0
**ìƒíƒœ**: ê³„íš ìˆ˜ë¦½ ì™„ë£Œ, ì‹¤í–‰ ëŒ€ê¸°

---

## ëª©ì°¨

1. [ê°œìš” ë° ëª©í‘œ](#1-ê°œìš”-ë°-ëª©í‘œ)
2. [í˜„ìž¬ í™˜ê²½ ë¶„ì„](#2-í˜„ìž¬-í™˜ê²½-ë¶„ì„)
3. [ì•„í‚¤í…ì²˜ ì„¤ê³„](#3-ì•„í‚¤í…ì²˜-ì„¤ê³„)
4. [ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš](#4-ë‹¨ê³„ë³„-ì‹¤í–‰-ê³„íš)
5. [ì„¤ì • íŒŒì¼ ë³€ê²½](#5-ì„¤ì •-íŒŒì¼-ë³€ê²½)
6. [ìµœì¢… íŒŒì¼ êµ¬ì¡°](#6-ìµœì¢…-íŒŒì¼-êµ¬ì¡°)
7. [ê²€ì¦ ë° ì„±ê³µ ê¸°ì¤€](#7-ê²€ì¦-ë°-ì„±ê³µ-ê¸°ì¤€)
8. [ë¬¸ì„œí™” ê³„íš](#8-ë¬¸ì„œí™”-ê³„íš)
9. [ì§„í–‰ ìƒí™© ì¶”ì ](#9-ì§„í–‰-ìƒí™©-ì¶”ì )
10. [ì°¸ê³  ìžë£Œ](#10-ì°¸ê³ -ìžë£Œ)

---

## 1. ê°œìš” ë° ëª©í‘œ

### 1.1 Phase 3ì˜ ëª©ì 

Phase 4 (CI/CD íŒŒì´í”„ë¼ì¸)ë¥¼ ì™„ë£Œí•œ ì‹œì ì—ì„œ, Phase 3ì€ **í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆê³¼ ê¸°ëŠ¥ì„ ê³ ë„í™”**í•˜ì—¬ í”„ë¡œë•ì…˜ê¸‰ ML ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „ì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

### 1.2 ì „ì²´ ëª©í‘œ

1. **í’ˆì§ˆ ê°•í™”**: í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 75%+, mypy strict ëª¨ë“œ í†µê³¼
2. **MLflow ê³ ê¸‰ ê¸°ëŠ¥**: Model Registryë¥¼ í†µí•œ ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬
3. **í•™ìŠµ ìµœì í™”**: Optunaë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìžë™ íŠœë‹
4. **í™•ìž¥ì„± ì¤€ë¹„**: DDP ë¶„ì‚° í•™ìŠµ ì½”ë“œ êµ¬ì¡° ì™„ì„±

### 1.3 ìš°ì„ ìˆœìœ„ ë° ì¼ì •

| ìš°ì„ ìˆœìœ„ | Phase | ìž‘ì—… ë‚´ìš© | ì˜ˆìƒ ê¸°ê°„ |
|---------|-------|----------|----------|
| 1 | 3.1 | í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ê°œì„  | 1-2ì¼ |
| 2 | 3.2 | íƒ€ìž… ì•ˆì „ì„± ê°•í™” | 1ì¼ |
| 3 | 3.5 | MLflow Model Registry | 2-3ì¼ |
| 4 | 3.4 | Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | 3-4ì¼ |
| 5 | 3.3 | DDP ë¶„ì‚° í•™ìŠµ (ì½”ë“œë§Œ) | 2-3ì¼ |

**ì´ ì˜ˆìƒ ê¸°ê°„**: 3-4ì£¼

### 1.4 ì„±ê³µ ê¸°ì¤€ ìš”ì•½

- âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 75% ì´ìƒ
- âœ… mypy strict ëª¨ë“œ 100% í†µê³¼
- âœ… MLflow Model Registry í†µí•œ ëª¨ë¸ ìžë™ ê´€ë¦¬
- âœ… Optunaë¡œ CIFAR-10 ì •í™•ë„ 90%+ ë‹¬ì„±
- âœ… DDP ì½”ë“œ êµ¬ì¡° ì™„ì„± (í…ŒìŠ¤íŠ¸ëŠ” ì¶”í›„ í´ë¼ìš°ë“œì—ì„œ)

---

## 2. í˜„ìž¬ í™˜ê²½ ë¶„ì„

### 2.1 ê°•ì 

**ì¸í”„ë¼ ë° ì•„í‚¤í…ì²˜**:
- âœ… Pydantic ê¸°ë°˜ íƒ€ìž… ì•ˆì „ ì„¤ì • ê´€ë¦¬
- âœ… MLflow ì™„ì „ í†µí•© (PostgreSQL + MinIO)
- âœ… Docker í‘œì¤€í™” ì™„ë£Œ (Python ë²„ì „ ë¬´ê´€)
- âœ… CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ (GitHub Actions)
- âœ… ëª¨ë“ˆí™”ëœ ì½”ë“œ êµ¬ì¡° (`src/config`, `src/models`, `src/training`, `src/data`)

**í…ŒìŠ¤íŠ¸ ë° í’ˆì§ˆ**:
- âœ… 52ê°œ ìžë™í™” í…ŒìŠ¤íŠ¸
- âœ… 56.61% ì½”ë“œ ì»¤ë²„ë¦¬ì§€ (ëª©í‘œ 50% ì´ˆê³¼)
- âœ… ì½”ë“œ í’ˆì§ˆ ìžë™í™” (Black, isort, flake8, mypy)
- âœ… ë³´ì•ˆ ìŠ¤ìº” (Trivy, Bandit)

### 2.2 ê°œì„  í•„ìš” ì‚¬í•­

**í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**:
- âš ï¸ `src/training/evaluate.py`: 18.02% (ê°œì„  í•„ìš”)
- âš ï¸ `src/training/train.py`: 52.00% (ì–‘í˜¸)
- âš ï¸ `src/data/dataset.py`: 51.79% (ì–‘í˜¸)

**íƒ€ìž… ì•ˆì „ì„±**:
- âš ï¸ ì¼ë¶€ í•¨ìˆ˜ì— íƒ€ìž… ížŒíŠ¸ ëˆ„ë½
- âš ï¸ mypy strict ëª¨ë“œ ë¹„í™œì„±í™” ìƒíƒœ
- âš ï¸ ë³µìž¡í•œ íƒ€ìž…ì— ëŒ€í•œ ëª…ì‹œì  ì •ì˜ ë¶€ì¡±

**ê¸°ëŠ¥**:
- âŒ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ìˆ˜ë™
- âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìˆ˜ë™ ì„¤ì •
- âŒ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í•™ìŠµë§Œ ì§€ì›

### 2.3 í™˜ê²½ ì œì•½ì‚¬í•­

**í•˜ë“œì›¨ì–´**:
- MacBook M2 (MPS backend, single GPU)
- MPSëŠ” PyTorch DDP ë¯¸ì§€ì›
- ë¡œì»¬ CPU DDPëŠ” í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ëŠë¦¼

**ê°œë°œ í™˜ê²½**:
- ë¡œì»¬ ê°œë°œ ì¤‘ì‹¬
- í´ë¼ìš°ë“œ í™˜ê²½ ë¯¸êµ¬ì¶• (Kubernetes í™•ìž¥ì€ Phase 7-8)

**DDP ì œì•½**:
- M2 í™˜ê²½ì—ì„œ ì‹¤ì œ multi-GPU í…ŒìŠ¤íŠ¸ ë¶ˆê°€
- DDP ì½”ë“œ êµ¬ì¡°ë§Œ ì™„ì„±, ì‹¤ì œ í…ŒìŠ¤íŠ¸ëŠ” ì¶”í›„ í´ë¼ìš°ë“œì—ì„œ ìˆ˜í–‰

---

## 3. ì•„í‚¤í…ì²˜ ì„¤ê³„

### 3.1 ì „ì²´ ëª¨ë“ˆ êµ¬ì¡°

```
src/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py          # ìˆ˜ì •: Optuna, DDP ì„¤ì • ì¶”ê°€
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py           # ê¸°ì¡´
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ vision_model.py      # ê¸°ì¡´
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ types.py             # ì‹ ê·œ: TypedDict, Protocol ì •ì˜
â”‚   â”œâ”€â”€ train.py             # ìˆ˜ì •: Registry, DDP í†µí•©
â”‚   â”œâ”€â”€ evaluate.py          # ê¸°ì¡´
â”‚   â”œâ”€â”€ registry.py          # ì‹ ê·œ: MLflow Model Registry
â”‚   â”œâ”€â”€ tuning.py            # ì‹ ê·œ: Optuna íŠœë‹
â”‚   â”œâ”€â”€ distributed.py       # ì‹ ê·œ: DDP ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ train_distributed.py # ì‹ ê·œ: DDP ì§„ìž…ì 
â””â”€â”€ utils/                   # ì‹ ê·œ (í•„ìš”ì‹œ)
    â””â”€â”€ __init__.py

tests/
â”œâ”€â”€ test_config.py
â”œâ”€â”€ test_data.py
â”œâ”€â”€ test_models.py
â”œâ”€â”€ test_training.py
â”œâ”€â”€ test_e2e.py
â”œâ”€â”€ test_evaluate_extended.py   # ì‹ ê·œ: Phase 3.1
â”œâ”€â”€ test_dataset_extended.py    # ì‹ ê·œ: Phase 3.1
â”œâ”€â”€ test_training_extended.py   # ì‹ ê·œ: Phase 3.1
â”œâ”€â”€ test_registry.py            # ì‹ ê·œ: Phase 3.5
â”œâ”€â”€ test_tuning.py              # ì‹ ê·œ: Phase 3.4
â””â”€â”€ test_distributed.py         # ì‹ ê·œ: Phase 3.3

docs/
â”œâ”€â”€ phase3_plan.md              # ë³¸ ë¬¸ì„œ
â”œâ”€â”€ model_registry.md           # ì‹ ê·œ: Phase 3.5
â”œâ”€â”€ hyperparameter_tuning.md    # ì‹ ê·œ: Phase 3.4
â””â”€â”€ distributed_training.md     # ì‹ ê·œ: Phase 3.3
```

### 3.2 ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

#### 3.2.1 TypedDict ë° Protocol (Phase 3.2)

```python
# src/training/types.py
from typing import TypedDict, Protocol, Any
from torch import nn, optim

class TrainingMetrics(TypedDict):
    """Training metrics for one epoch."""
    loss: float
    accuracy: float
    val_loss: float
    val_accuracy: float

class EvaluationMetrics(TypedDict):
    """Evaluation metrics from evaluate module."""
    accuracy: float
    confusion_matrix: Any  # numpy array
    per_class_metrics: dict[str, dict[str, float]]

class OptimizerProtocol(Protocol):
    """Protocol for PyTorch optimizers."""
    def zero_grad(self) -> None: ...
    def step(self) -> None: ...

class LRSchedulerProtocol(Protocol):
    """Protocol for learning rate schedulers."""
    def step(self, metrics: float | None = None) -> None: ...
    def get_last_lr(self) -> list[float]: ...
```

#### 3.2.2 MLflow Model Registry (Phase 3.5)

```python
# src/training/registry.py
from typing import Optional, Literal
import mlflow
from mlflow.tracking import MlflowClient
from mlflow.entities.model_registry import ModelVersion

Stage = Literal["None", "Staging", "Production", "Archived"]

class ModelRegistry:
    """MLflow Model Registry manager for model lifecycle."""

    def __init__(self, model_name: str = "vision-classifier"):
        """Initialize Model Registry.

        Args:
            model_name: Registered model name in MLflow
        """
        self.client = MlflowClient()
        self.model_name = model_name

    def register_model(
        self,
        run_id: str,
        model_path: str = "model",
        description: Optional[str] = None
    ) -> ModelVersion:
        """Register model from MLflow run.

        Args:
            run_id: MLflow run ID
            model_path: Artifact path of the model
            description: Model description

        Returns:
            Registered ModelVersion
        """
        model_uri = f"runs:/{run_id}/{model_path}"

        # Register model
        mv = mlflow.register_model(
            model_uri=model_uri,
            name=self.model_name
        )

        # Update description if provided
        if description:
            self.client.update_model_version(
                name=self.model_name,
                version=mv.version,
                description=description
            )

        return mv

    def promote_model(
        self,
        version: int,
        stage: Stage,
        archive_existing: bool = True
    ) -> None:
        """Promote model to a stage.

        Args:
            version: Model version number
            stage: Target stage (Staging, Production, Archived)
            archive_existing: Archive existing models in target stage
        """
        if archive_existing and stage in ["Staging", "Production"]:
            # Archive existing models in target stage
            existing = self.client.get_latest_versions(
                name=self.model_name,
                stages=[stage]
            )
            for mv in existing:
                self.client.transition_model_version_stage(
                    name=self.model_name,
                    version=mv.version,
                    stage="Archived"
                )

        # Promote new model
        self.client.transition_model_version_stage(
            name=self.model_name,
            version=version,
            stage=stage
        )

    def get_latest_model(
        self,
        stage: Stage = "Production"
    ) -> Optional[ModelVersion]:
        """Get latest model version in a stage.

        Args:
            stage: Model stage

        Returns:
            Latest ModelVersion or None
        """
        versions = self.client.get_latest_versions(
            name=self.model_name,
            stages=[stage]
        )
        return versions[0] if versions else None

    def compare_models(
        self,
        versions: list[int],
        metric_name: str = "val_accuracy"
    ) -> dict[int, float]:
        """Compare models by metric.

        Args:
            versions: List of version numbers
            metric_name: Metric to compare

        Returns:
            Dictionary mapping version to metric value
        """
        results = {}
        for version in versions:
            mv = self.client.get_model_version(
                name=self.model_name,
                version=str(version)
            )
            run = self.client.get_run(mv.run_id)
            metric_value = run.data.metrics.get(metric_name, 0.0)
            results[version] = metric_value

        return results

    def rollback_to_version(
        self,
        version: int,
        stage: Stage = "Production"
    ) -> None:
        """Rollback to a specific model version.

        Args:
            version: Version to rollback to
            stage: Target stage
        """
        self.promote_model(version, stage, archive_existing=True)
```

#### 3.2.3 Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Phase 3.4)

```python
# src/training/tuning.py
from typing import Optional
import optuna
from optuna.integration.mlflow import MLflowCallback
import mlflow

from src.config import get_settings
from src.training.train import train_model

class OptunaObjective:
    """Optuna objective for hyperparameter tuning."""

    def __init__(self, n_epochs: int = 10):
        """Initialize objective.

        Args:
            n_epochs: Number of epochs per trial
        """
        self.n_epochs = n_epochs
        self.settings = get_settings()

    def __call__(self, trial: optuna.Trial) -> float:
        """Objective function for Optuna.

        Args:
            trial: Optuna trial

        Returns:
            Validation accuracy
        """
        # Suggest hyperparameters
        lr = trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True)
        batch_size = trial.suggest_categorical("batch_size", [32, 64, 128, 256])
        weight_decay = trial.suggest_float("weight_decay", 1e-6, 1e-3, log=True)
        optimizer_name = trial.suggest_categorical("optimizer", ["adam", "sgd", "adamw"])

        # Update settings for this trial
        self.settings.learning_rate = lr
        self.settings.batch_size = batch_size
        self.settings.weight_decay = weight_decay
        self.settings.epochs = self.n_epochs

        # Train model
        try:
            # Create nested MLflow run for this trial
            with mlflow.start_run(nested=True, run_name=f"trial_{trial.number}"):
                # Log trial params
                mlflow.log_params(trial.params)

                # Train (returns run_id, but we get metrics from current run)
                train_model()

                # Get validation accuracy from current run
                run = mlflow.active_run()
                if run:
                    client = mlflow.tracking.MlflowClient()
                    metrics = client.get_run(run.info.run_id).data.metrics
                    val_accuracy = metrics.get("val_accuracy", 0.0)
                else:
                    val_accuracy = 0.0

                return val_accuracy

        except Exception as e:
            # Log error and return 0
            mlflow.log_param("error", str(e))
            return 0.0

def run_hyperparameter_search(
    n_trials: int = 50,
    n_epochs_per_trial: int = 10,
    study_name: Optional[str] = None,
    storage: str = "sqlite:///optuna.db"
) -> optuna.Study:
    """Run Optuna hyperparameter search.

    Args:
        n_trials: Number of trials to run
        n_epochs_per_trial: Epochs per trial
        study_name: Optuna study name
        storage: Optuna storage URL

    Returns:
        Completed Optuna study
    """
    settings = get_settings()

    if study_name is None:
        study_name = f"{settings.experiment_name}-tuning"

    # Create MLflow callback
    mlflc = MLflowCallback(
        tracking_uri=settings.mlflow_tracking_uri,
        metric_name="val_accuracy"
    )

    # Create or load study
    study = optuna.create_study(
        study_name=study_name,
        direction="maximize",
        storage=storage,
        load_if_exists=True,
        sampler=optuna.samplers.TPESampler(seed=42)
    )

    # Run optimization within MLflow parent run
    with mlflow.start_run(run_name=f"optuna-{study_name}"):
        mlflow.log_param("n_trials", n_trials)
        mlflow.log_param("n_epochs_per_trial", n_epochs_per_trial)

        # Optimize
        study.optimize(
            OptunaObjective(n_epochs=n_epochs_per_trial),
            n_trials=n_trials,
            n_jobs=1,  # M2ì—ì„œëŠ” 1 ê¶Œìž¥
            callbacks=[mlflc]
        )

        # Log best params
        mlflow.log_params({f"best_{k}": v for k, v in study.best_params.items()})
        mlflow.log_metric("best_val_accuracy", study.best_value)

        # Save optimization history plot
        try:
            import matplotlib.pyplot as plt
            fig = optuna.visualization.matplotlib.plot_optimization_history(study)
            mlflow.log_figure(fig, "optimization_history.png")
            plt.close(fig)

            fig = optuna.visualization.matplotlib.plot_param_importances(study)
            mlflow.log_figure(fig, "param_importances.png")
            plt.close(fig)
        except Exception:
            pass  # Visualization errors are non-critical

    return study

if __name__ == "__main__":
    study = run_hyperparameter_search(n_trials=50)
    print(f"Best trial: {study.best_trial.number}")
    print(f"Best params: {study.best_params}")
    print(f"Best value: {study.best_value}")
```

#### 3.2.4 DDP ë¶„ì‚° í•™ìŠµ ìœ í‹¸ë¦¬í‹° (Phase 3.3)

```python
# src/training/distributed.py
import os
import torch
import torch.distributed as dist
from typing import Optional

def setup_distributed(
    backend: Optional[str] = None,
    init_method: str = "env://"
) -> None:
    """Initialize distributed training.

    Args:
        backend: Backend to use (nccl, gloo, mpi). Auto-detect if None.
        init_method: Initialization method
    """
    if not dist.is_available():
        raise RuntimeError("Distributed training not available")

    # Auto-detect backend if not specified
    if backend is None:
        if torch.cuda.is_available():
            backend = "nccl"  # Best for GPU
        elif torch.backends.mps.is_available():
            # MPS doesn't support DDP, use gloo on CPU
            backend = "gloo"
            print("âš ï¸  WARNING: MPS backend doesn't support DDP. Using CPU with gloo backend.")
        else:
            backend = "gloo"  # CPU fallback

    # Get rank and world size from environment variables
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    # Initialize process group
    dist.init_process_group(
        backend=backend,
        init_method=init_method,
        rank=rank,
        world_size=world_size
    )

    print(f"Initialized DDP: rank={rank}, world_size={world_size}, backend={backend}")

def cleanup_distributed() -> None:
    """Cleanup distributed training."""
    if dist.is_initialized():
        dist.destroy_process_group()

def is_distributed() -> bool:
    """Check if running in distributed mode."""
    return dist.is_available() and dist.is_initialized()

def is_main_process() -> bool:
    """Check if current process is main (rank 0)."""
    return not is_distributed() or dist.get_rank() == 0

def get_rank() -> int:
    """Get current process rank."""
    return dist.get_rank() if is_distributed() else 0

def get_world_size() -> int:
    """Get total number of processes."""
    return dist.get_world_size() if is_distributed() else 1

def reduce_dict(input_dict: dict[str, float]) -> dict[str, float]:
    """Reduce dictionary values across all processes.

    Args:
        input_dict: Dictionary with float values

    Returns:
        Averaged dictionary
    """
    if not is_distributed():
        return input_dict

    world_size = get_world_size()
    keys = sorted(input_dict.keys())
    values = [input_dict[k] for k in keys]

    # Convert to tensor
    values_tensor = torch.tensor(values, dtype=torch.float32)

    # All-reduce
    dist.all_reduce(values_tensor, op=dist.ReduceOp.SUM)
    values_tensor /= world_size

    # Convert back to dict
    return {k: v.item() for k, v in zip(keys, values_tensor)}
```

---

## 4. ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš

### Phase 3.1: í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ê°œì„ 

**ëª©í‘œ**: ì „ì²´ ì»¤ë²„ë¦¬ì§€ 56.61% â†’ 75%+

**ì˜ˆìƒ ì†Œìš”**: 1-2ì¼

#### ìž‘ì—… 1: `tests/test_evaluate_extended.py` ìƒì„±

**ëª©ì **: `src/training/evaluate.py` ì»¤ë²„ë¦¬ì§€ 18% â†’ 70%+

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**:
```python
# tests/test_evaluate_extended.py
import pytest
import torch
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch

from src.training.evaluate import evaluate_model

class TestEvaluateExtended:
    """Extended tests for evaluate module."""

    def test_confusion_matrix_generation(self, tmp_path):
        """Test confusion matrix is generated correctly."""
        # Mock model, dataloader, etc.
        # ...

    def test_per_class_metrics_calculation(self):
        """Test per-class precision/recall/f1 calculation."""
        # ...

    def test_visualization_saving(self, tmp_path):
        """Test that confusion matrix visualization is saved."""
        # ...

    def test_mlflow_metrics_logging(self):
        """Test that metrics are logged to MLflow."""
        with patch('mlflow.log_metric') as mock_log:
            # ...
            assert mock_log.called
```

#### ìž‘ì—… 2: `tests/test_dataset_extended.py` ìƒì„±

**ëª©ì **: `src/data/dataset.py` ì»¤ë²„ë¦¬ì§€ 51% â†’ 80%+

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**:
```python
# tests/test_dataset_extended.py
class TestDatasetExtended:
    """Extended tests for dataset module."""

    def test_train_val_split_ratio(self):
        """Test train/val split ratio is correct."""
        # ...

    def test_dataloader_num_workers(self):
        """Test DataLoader with different num_workers."""
        # ...

    def test_augmentation_pipeline(self):
        """Test data augmentation is applied correctly."""
        # ...

    def test_multiple_datasets(self):
        """Test loading CIFAR-10, CIFAR-100, FashionMNIST."""
        # ...
```

#### ìž‘ì—… 3: `tests/test_training_extended.py` ìƒì„±

**ëª©ì **: `src/training/train.py` ì»¤ë²„ë¦¬ì§€ 52% â†’ 80%+

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**:
```python
# tests/test_training_extended.py
class TestTrainingExtended:
    """Extended tests for training module."""

    def test_early_stopping_trigger(self):
        """Test early stopping is triggered correctly."""
        # ...

    def test_checkpoint_save_load(self, tmp_path):
        """Test model checkpoint save and load."""
        # ...

    def test_learning_rate_scheduler(self):
        """Test LR scheduler updates learning rate."""
        # ...
```

#### ê²€ì¦ ë°©ë²•

```bash
# í™•ìž¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
poetry run pytest tests/test_*_extended.py -v

# ì „ì²´ ì»¤ë²„ë¦¬ì§€ í™•ì¸
poetry run pytest --cov=src --cov-report=term --cov-report=html

# ì„±ê³µ ê¸°ì¤€
# - Overall coverage â‰¥ 75%
# - evaluate.py â‰¥ 70%
# - dataset.py â‰¥ 80%
# - train.py â‰¥ 80%
```

---

### Phase 3.2: íƒ€ìž… ì•ˆì „ì„± ê°•í™”

**ëª©í‘œ**: mypy strict ëª¨ë“œ 100% í†µê³¼

**ì˜ˆìƒ ì†Œìš”**: 1ì¼

#### ìž‘ì—… 1: `src/training/types.py` ìƒì„±

ìœ„ì˜ [3.2.1 TypedDict ë° Protocol](#321-typeddict-ë°-protocol-phase-32) ì°¸ì¡°

#### ìž‘ì—… 2: ëª¨ë“  í•¨ìˆ˜ì— íƒ€ìž… ížŒíŠ¸ ì¶”ê°€

**íŒŒì¼ ëª©ë¡**:
- `src/training/train.py`
- `src/training/evaluate.py`
- `src/data/dataset.py`
- `src/models/vision_model.py`
- `src/config/settings.py`

**ì˜ˆì‹œ**:
```python
# Before
def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    ...

# After
def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device,
    epoch: int,
) -> tuple[float, float]:
    ...
```

#### ìž‘ì—… 3: `pyproject.toml` mypy ì„¤ì •

```toml
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_any_unimported = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true

[[tool.mypy.overrides]]
module = [
    "matplotlib.*",
    "seaborn.*",
    "sklearn.*",
    "mlflow.*",
]
ignore_missing_imports = true
```

#### ìž‘ì—… 4: CI/CD ì—…ë°ì´íŠ¸

`.github/workflows/test.yml`:
```yaml
- name: Run mypy (type checking)
  run: |
    poetry run mypy src/
  continue-on-error: false  # strict ëª¨ë“œ, ì‹¤íŒ¨ ì‹œ CI ì¤‘ë‹¨
```

#### ê²€ì¦ ë°©ë²•

```bash
# ë¡œì»¬ mypy ê²€ì‚¬
poetry run mypy src/ --strict

# ì„±ê³µ ê¸°ì¤€
# - mypy: 0 errors
# - ëª¨ë“  public í•¨ìˆ˜ íƒ€ìž… ížŒíŠ¸ ì¡´ìž¬
# - CI/CD mypy strict í†µê³¼
```

---

### Phase 3.5: MLflow Model Registry

**ëª©í‘œ**: ëª¨ë¸ ìƒëª…ì£¼ê¸° ìžë™ ê´€ë¦¬

**ì˜ˆìƒ ì†Œìš”**: 2-3ì¼

#### ìž‘ì—… ë‚´ìš©

1. **`src/training/registry.py` ìƒì„±**
   ìœ„ì˜ [3.2.2 MLflow Model Registry](#322-mlflow-model-registry-phase-35) ì°¸ì¡°

2. **`src/training/train.py` ìˆ˜ì •**
   í•™ìŠµ ì™„ë£Œ í›„ ìžë™ ëª¨ë¸ ë“±ë¡:
   ```python
   from src.training.registry import ModelRegistry

   def train_model() -> str:
       # ... í•™ìŠµ ì½”ë“œ ...

       # í•™ìŠµ ì™„ë£Œ í›„
       run_id = mlflow.active_run().info.run_id

       # ëª¨ë¸ ë“±ë¡
       registry = ModelRegistry(model_name="vision-classifier")
       mv = registry.register_model(
           run_id=run_id,
           description=f"Model trained on {settings.dataset}"
       )

       # ìµœê³  ì„±ëŠ¥ì´ë©´ Stagingìœ¼ë¡œ ìŠ¹ê²©
       if best_val_acc > 0.85:  # ìž„ê³„ê°’
           registry.promote_model(
               version=int(mv.version),
               stage="Staging"
           )

       return run_id
   ```

3. **`tests/test_registry.py` ìƒì„±**
   ```python
   class TestModelRegistry:
       def test_register_model(self):
           """Test model registration."""
           # ...

       def test_promote_to_staging(self):
           """Test promoting model to Staging."""
           # ...

       def test_get_latest_production_model(self):
           """Test retrieving latest production model."""
           # ...
   ```

4. **`docs/model_registry.md` ë¬¸ì„œ ìƒì„±**

#### MLflow UI í™œìš©

```bash
# MLflow ì„œë²„ ì‹œìž‘
make up

# ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
open http://localhost:5001

# Models íƒ­:
# - ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡
# - ë²„ì „ë³„ ë©”íŠ¸ë¦­ ë¹„êµ
# - Stage ì „í™˜ ì´ë ¥
# - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```

#### ê²€ì¦ ë°©ë²•

```bash
# í•™ìŠµ ì‹¤í–‰
make train

# MLflow UI í™•ì¸
# - Models íƒ­ì—ì„œ "vision-classifier" í™•ì¸
# - ìµœì‹  ë²„ì „ í™•ì¸
# - Stage í™•ì¸ (None ë˜ëŠ” Staging)

# í…ŒìŠ¤íŠ¸
poetry run pytest tests/test_registry.py -v
```

---

### Phase 3.4: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

**ëª©í‘œ**: ìžë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¡œ ì •í™•ë„ 90%+ ë‹¬ì„±

**ì˜ˆìƒ ì†Œìš”**: 3-4ì¼

#### ìž‘ì—… 1: ì˜ì¡´ì„± ì¶”ê°€

```bash
poetry add optuna optuna-integration[mlflow]
```

#### ìž‘ì—… 2: `src/training/tuning.py` ìƒì„±

ìœ„ì˜ [3.2.3 Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹](#323-optuna-í•˜ì´í¼íŒŒë¼ë¯¸í„°-íŠœë‹-phase-34) ì°¸ì¡°

#### ìž‘ì—… 3: `src/config/settings.py` ìˆ˜ì •

```python
class Settings(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì • ...

    # Optuna ì„¤ì •
    optuna_n_trials: int = Field(
        default=50,
        description="Number of Optuna trials",
        gt=0,
    )
    optuna_n_epochs_per_trial: int = Field(
        default=10,
        description="Epochs per Optuna trial",
        gt=0,
    )
    optuna_study_name: str = Field(
        default="vision-tuning",
        description="Optuna study name",
    )
    optuna_storage: str = Field(
        default="sqlite:///optuna.db",
        description="Optuna storage URL",
    )
```

#### ìž‘ì—… 4: Makefile ëª…ë ¹ì–´ ì¶”ê°€

```makefile
.PHONY: tune
tune: ## Run Optuna hyperparameter tuning
	@echo "Starting Optuna hyperparameter tuning..."
	poetry run python -m src.training.tuning
```

#### ìž‘ì—… 5: `tests/test_tuning.py` ìƒì„±

```python
class TestOptunaTuning:
    def test_objective_function(self):
        """Test OptunaObjective callable."""
        # ...

    def test_mlflow_integration(self):
        """Test MLflow logging in trials."""
        # ...

    def test_best_params_extraction(self):
        """Test best params are logged."""
        # ...
```

#### ìž‘ì—… 6: `docs/hyperparameter_tuning.md` ë¬¸ì„œ ìƒì„±

#### ê²€ì¦ ë°©ë²•

```bash
# Optuna íŠœë‹ ì‹¤í–‰
make tune

# MLflow UIì—ì„œ í™•ì¸
# - "optuna-vision-tuning" parent run
# - 50ê°œ nested runs (ê° trial)
# - ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸
# - Optimization history ì‹œê°í™” í™•ì¸

# ì„±ê³µ ê¸°ì¤€
# - 50 trials ì™„ë£Œ
# - ìµœì  íŒŒë¼ë¯¸í„° ë°œê²¬
# - CIFAR-10 val_accuracy 90%+ (ëª©í‘œ)
```

---

### Phase 3.3: DDP ë¶„ì‚° í•™ìŠµ

**ëª©í‘œ**: DDP ì½”ë“œ êµ¬ì¡° ì™„ì„±, í…ŒìŠ¤íŠ¸ëŠ” ì¶”í›„ í´ë¼ìš°ë“œì—ì„œ

**ì˜ˆìƒ ì†Œìš”**: 2-3ì¼

**âš ï¸ ì¤‘ìš”**: M2 Mac ì œì•½ìœ¼ë¡œ ì‹¤ì œ multi-GPU í…ŒìŠ¤íŠ¸ëŠ” [TODO.md](../TODO.md)ì— ëª…ì‹œ

#### ìž‘ì—… 1: `src/training/distributed.py` ìƒì„±

ìœ„ì˜ [3.2.4 DDP ë¶„ì‚° í•™ìŠµ ìœ í‹¸ë¦¬í‹°](#324-ddp-ë¶„ì‚°-í•™ìŠµ-ìœ í‹¸ë¦¬í‹°-phase-33) ì°¸ì¡°

#### ìž‘ì—… 2: `src/training/train.py` ìˆ˜ì •

```python
from src.training.distributed import (
    is_distributed,
    is_main_process,
    setup_distributed,
    cleanup_distributed,
)

def train_model() -> str:
    settings = get_settings()

    # Setup distributed if enabled
    if settings.distributed:
        setup_distributed()

    try:
        # ... ê¸°ì¡´ í•™ìŠµ ì½”ë“œ ...

        # MLflow ë¡œê¹…ì€ main processë§Œ
        if is_main_process():
            mlflow.log_metric("loss", loss)
            mlflow.log_metric("accuracy", acc)

        # ... í•™ìŠµ ê³„ì† ...

    finally:
        # Cleanup distributed
        if settings.distributed:
            cleanup_distributed()

    return run_id
```

#### ìž‘ì—… 3: `src/training/train_distributed.py` ìƒì„±

```python
# src/training/train_distributed.py
"""Distributed training entry point."""

from src.training.train import train_model
from src.training.distributed import setup_distributed, cleanup_distributed

def main():
    """Main function for distributed training."""
    setup_distributed()

    try:
        train_model()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()
```

#### ìž‘ì—… 4: `src/config/settings.py` ìˆ˜ì •

```python
class Settings(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì • ...

    # DDP ì„¤ì •
    distributed: bool = Field(
        default=False,
        description="Enable distributed training",
    )
    backend: Literal["nccl", "gloo", "mpi"] = Field(
        default="nccl",
        description="Distributed backend",
    )
```

#### ìž‘ì—… 5: Makefile ëª…ë ¹ì–´ ì¶”ê°€

```makefile
.PHONY: train-ddp
train-ddp: ## Run distributed training (local CPU test only)
	@echo "âš ï¸  WARNING: M2 MPS doesn't support DDP"
	@echo "Using CPU gloo backend for basic testing only"
	@echo "For actual multi-GPU training, see TODO.md"
	torchrun --nproc_per_node=2 src/training/train_distributed.py
```

#### ìž‘ì—… 6: `tests/test_distributed.py` ìƒì„±

```python
class TestDistributed:
    """Basic distributed utility tests."""

    def test_setup_cleanup(self):
        """Test distributed setup and cleanup."""
        # Basic initialization test only
        # Actual DDP training test in TODO.md
        # ...
```

#### ìž‘ì—… 7: `docs/distributed_training.md` ë¬¸ì„œ ìƒì„±

**í¬í•¨ ë‚´ìš©**:
- DDP ì½”ë“œ êµ¬ì¡° ì„¤ëª…
- ë¡œì»¬ CPU í…ŒìŠ¤íŠ¸ ë°©ë²•
- **TODO ì„¹ì…˜**: í´ë¼ìš°ë“œ GPU í…ŒìŠ¤íŠ¸ ê³„íš

#### ìž‘ì—… 8: `TODO.md` ì—…ë°ì´íŠ¸

DDP í´ë¼ìš°ë“œ í…ŒìŠ¤íŠ¸ í•­ëª© ì¶”ê°€ (ìžì„¸í•œ ë‚´ìš©ì€ ì•„ëž˜ [TODO.md](#todomd) ì„¹ì…˜ ì°¸ì¡°)

#### ê²€ì¦ ë°©ë²•

```bash
# ë¡œì»¬ CPU DDP í…ŒìŠ¤íŠ¸ (ê¸°ë³¸ ë™ìž‘ë§Œ)
make train-ddp

# ì„±ê³µ ê¸°ì¤€
# - 2 processes ì‹œìž‘
# - ê° processê°€ rank 0, 1 í• ë‹¹
# - ê¸°ë³¸ í•™ìŠµ ë™ìž‘
# - ì—ëŸ¬ ì—†ì´ ì¢…ë£Œ

# âš ï¸ ì‹¤ì œ multi-GPU í…ŒìŠ¤íŠ¸ëŠ” TODO.md ì°¸ì¡°
```

---

## 5. ì„¤ì • íŒŒì¼ ë³€ê²½

### 5.1 pyproject.toml

```toml
[tool.poetry.dependencies]
python = ">=3.9,<3.14"
# ... ê¸°ì¡´ ì˜ì¡´ì„± ...

# Phase 3.4: Optuna
optuna = "^3.5.0"
optuna-integration = {extras = ["mlflow"], version = "^3.5.0"}

[tool.poetry.group.dev.dependencies]
# ... ê¸°ì¡´ dev ì˜ì¡´ì„± ...

# Phase 3.2: mypy strict ì„¤ì •
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_any_unimported = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true

# External libraries without type stubs
[[tool.mypy.overrides]]
module = [
    "matplotlib.*",
    "seaborn.*",
    "sklearn.*",
    "mlflow.*",
    "optuna.*",
]
ignore_missing_imports = true

# Phase 3.1: ì»¤ë²„ë¦¬ì§€ ìž„ê³„ê°’ ìƒí–¥
[tool.pytest.ini_options]
minversion = "7.0"
addopts = """
  --cov=src
  --cov-report=html
  --cov-report=term
  --cov-report=xml
  --cov-fail-under=75
  -v
"""
testpaths = ["tests"]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
]
```

### 5.2 .github/workflows/test.yml

```yaml
# Phase 3.2: mypy strict ê²€ì‚¬ í™œì„±í™”
- name: Run mypy (type checking)
  run: |
    poetry run mypy src/
  continue-on-error: false  # strict ëª¨ë“œì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ CI ì¤‘ë‹¨
```

### 5.3 Makefile

```makefile
# ... ê¸°ì¡´ ëª…ë ¹ì–´ ...

# Phase 3.1: í™•ìž¥ í…ŒìŠ¤íŠ¸
.PHONY: test-extended
test-extended: ## Run extended tests with coverage
	poetry run pytest tests/test_*_extended.py -v --cov=src

# Phase 3.4: Optuna íŠœë‹
.PHONY: tune
tune: ## Run Optuna hyperparameter tuning
	@echo "Starting Optuna hyperparameter tuning..."
	@echo "This will run $(OPTUNA_N_TRIALS) trials (default: 50)"
	poetry run python -m src.training.tuning

# Phase 3.3: DDP (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
.PHONY: train-ddp
train-ddp: ## Run distributed training (local CPU test only)
	@echo "âš ï¸  WARNING: M2 MPS doesn't support DDP"
	@echo "Using CPU gloo backend for basic testing only"
	@echo "For actual multi-GPU training, see TODO.md"
	torchrun --nproc_per_node=2 src/training/train_distributed.py
```

---

## 6. ìµœì¢… íŒŒì¼ êµ¬ì¡°

### 6.1 ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
mlflow-study/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ test.yml (ìˆ˜ì •: mypy strict)
â”‚       â”œâ”€â”€ docker.yml
â”‚       â””â”€â”€ release.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py (ìˆ˜ì •: Optuna, DDP ì„¤ì •)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ vision_model.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ types.py (ì‹ ê·œ: TypedDict, Protocol)
â”‚   â”‚   â”œâ”€â”€ train.py (ìˆ˜ì •: Registry, DDP í†µí•©)
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â”œâ”€â”€ registry.py (ì‹ ê·œ: Model Registry)
â”‚   â”‚   â”œâ”€â”€ tuning.py (ì‹ ê·œ: Optuna)
â”‚   â”‚   â”œâ”€â”€ distributed.py (ì‹ ê·œ: DDP ìœ í‹¸)
â”‚   â”‚   â””â”€â”€ train_distributed.py (ì‹ ê·œ: DDP ì§„ìž…ì )
â”‚   â””â”€â”€ utils/ (ì‹ ê·œ, í•„ìš”ì‹œ)
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_training.py
â”‚   â”œâ”€â”€ test_e2e.py
â”‚   â”œâ”€â”€ test_evaluate_extended.py (ì‹ ê·œ: 3.1)
â”‚   â”œâ”€â”€ test_dataset_extended.py (ì‹ ê·œ: 3.1)
â”‚   â”œâ”€â”€ test_training_extended.py (ì‹ ê·œ: 3.1)
â”‚   â”œâ”€â”€ test_registry.py (ì‹ ê·œ: 3.5)
â”‚   â”œâ”€â”€ test_tuning.py (ì‹ ê·œ: 3.4)
â”‚   â””â”€â”€ test_distributed.py (ì‹ ê·œ: 3.3)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ phase3_plan.md (ì‹ ê·œ: ë³¸ ë¬¸ì„œ)
â”‚   â”œâ”€â”€ model_registry.md (ì‹ ê·œ: 3.5)
â”‚   â”œâ”€â”€ hyperparameter_tuning.md (ì‹ ê·œ: 3.4)
â”‚   â””â”€â”€ distributed_training.md (ì‹ ê·œ: 3.3)
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.mlflow
â”œâ”€â”€ Makefile (ìˆ˜ì •: tune, train-ddp ì¶”ê°€)
â”œâ”€â”€ pyproject.toml (ìˆ˜ì •: Optuna, mypy strict)
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ README.md (ìˆ˜ì •: ë¬¸ì„œ ë§í¬ ì¶”ê°€)
â”œâ”€â”€ plan.md (ìˆ˜ì •: Phase 3 ìš”ì•½)
â”œâ”€â”€ TODO.md (ì‹ ê·œ: DDP í…ŒìŠ¤íŠ¸ ë“±)
â”œâ”€â”€ TESTING.md
â””â”€â”€ CICD.md
```

### 6.2 ìƒˆë¡œ ìƒì„±ë˜ëŠ” íŒŒì¼ ëª©ë¡

**ì½”ë“œ íŒŒì¼ (8ê°œ)**:
1. `src/training/types.py`
2. `src/training/registry.py`
3. `src/training/tuning.py`
4. `src/training/distributed.py`
5. `src/training/train_distributed.py`
6. `tests/test_evaluate_extended.py`
7. `tests/test_dataset_extended.py`
8. `tests/test_training_extended.py`
9. `tests/test_registry.py`
10. `tests/test_tuning.py`
11. `tests/test_distributed.py`

**ë¬¸ì„œ íŒŒì¼ (5ê°œ)**:
1. `docs/phase3_plan.md` (ë³¸ ë¬¸ì„œ)
2. `docs/model_registry.md`
3. `docs/hyperparameter_tuning.md`
4. `docs/distributed_training.md`
5. `TODO.md`

**ìˆ˜ì •ë˜ëŠ” íŒŒì¼ (6ê°œ)**:
1. `src/training/train.py`
2. `src/config/settings.py`
3. `pyproject.toml`
4. `Makefile`
5. `.github/workflows/test.yml`
6. `README.md`
7. `plan.md`

---

## 7. ê²€ì¦ ë° ì„±ê³µ ê¸°ì¤€

### 7.1 Phaseë³„ ê²€ì¦

#### Phase 3.1: í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
make test
# ë˜ëŠ”
poetry run pytest --cov=src --cov-report=term
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… Overall coverage: â‰¥ 75%
- âœ… `src/training/evaluate.py`: â‰¥ 70%
- âœ… `src/data/dataset.py`: â‰¥ 80%
- âœ… `src/training/train.py`: â‰¥ 80%
- âœ… CI/CD í†µê³¼

#### Phase 3.2: íƒ€ìž… ì•ˆì „ì„±

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
poetry run mypy src/ --strict
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… mypy: 0 errors
- âœ… ëª¨ë“  public í•¨ìˆ˜ íƒ€ìž… ížŒíŠ¸ ì¡´ìž¬
- âœ… CI/CD mypy strict í†µê³¼

#### Phase 3.5: Model Registry

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
# í•™ìŠµ ì‹¤í–‰
make train

# MLflow UI í™•ì¸
open http://localhost:5001

# í…ŒìŠ¤íŠ¸
poetry run pytest tests/test_registry.py -v
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ìžë™ ë“±ë¡
- âœ… MLflow UI Models íƒ­ì—ì„œ ëª¨ë¸ ë²„ì „ í™•ì¸ ê°€ëŠ¥
- âœ… Stage ì „í™˜ (None â†’ Staging â†’ Production) ë™ìž‘
- âœ… ëª¨ë¸ ë¹„êµ ë° ë¡¤ë°± ê¸°ëŠ¥ ë™ìž‘
- âœ… `tests/test_registry.py` ëª¨ë‘ í†µê³¼

#### Phase 3.4: Optuna íŠœë‹

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
# Optuna ì‹¤í–‰
make tune

# MLflow UI í™•ì¸
open http://localhost:5001
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… 50 trials ìžë™ ì‹¤í–‰ ì™„ë£Œ
- âœ… ê° trialì´ MLflow nested runìœ¼ë¡œ ê¸°ë¡
- âœ… ìµœì  íŒŒë¼ë¯¸í„° ìžë™ ë¡œê¹…
- âœ… CIFAR-10 ì •í™•ë„ í–¥ìƒ (ëª©í‘œ: 90%+)
- âœ… Optuna ì‹œê°í™” ìƒì„± (optimization_history.png ë“±)
- âœ… `tests/test_tuning.py` í†µê³¼

#### Phase 3.3: DDP

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
# ë¡œì»¬ CPU DDP í…ŒìŠ¤íŠ¸
make train-ddp

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸
poetry run pytest tests/test_distributed.py -v
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… DDP ì½”ë“œ êµ¬ì¡° ì™„ì„±
- âœ… `setup_distributed()` / `cleanup_distributed()` ë™ìž‘
- âœ… `is_main_process()` ì˜¬ë°”ë¥´ê²Œ ë™ìž‘
- âœ… ë¡œì»¬ CPU 2-process ê¸°ë³¸ ì‹¤í–‰ ì„±ê³µ
- âœ… `tests/test_distributed.py` ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í†µê³¼
- â³ `TODO.md`ì— í´ë¼ìš°ë“œ í…ŒìŠ¤íŠ¸ í•­ëª© ì¶”ê°€
- â³ ì‹¤ì œ multi-GPU í…ŒìŠ¤íŠ¸ëŠ” ë³´ë¥˜

### 7.2 ì „ì²´ Phase 3 ì„±ê³µ ê¸°ì¤€

**í•„ìˆ˜ í•­ëª©** (ëª¨ë‘ ì¶©ì¡± í•„ìš”):
- âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 75% ì´ìƒ
- âœ… mypy strict ëª¨ë“œ 0 errors
- âœ… 52ê°œ + ì‹ ê·œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼
- âœ… CI/CD íŒŒì´í”„ë¼ì¸ í†µê³¼
- âœ… MLflow Model Registry ë™ìž‘
- âœ… Optuna 50 trials ì™„ë£Œ

**ëª©í‘œ í•­ëª©** (ìµœì„ ì˜ ë…¸ë ¥):
- ðŸŽ¯ CIFAR-10 ì •í™•ë„ 90%+ (Optuna íŠœë‹ í›„)
- ðŸŽ¯ evaluate.py ì»¤ë²„ë¦¬ì§€ 70%+
- ðŸŽ¯ dataset.py ì»¤ë²„ë¦¬ì§€ 80%+
- ðŸŽ¯ train.py ì»¤ë²„ë¦¬ì§€ 80%+

**ì¶”í›„ í•­ëª©** (TODO.md):
- â³ DDP multi-GPU í´ë¼ìš°ë“œ í…ŒìŠ¤íŠ¸
- â³ í•™ìŠµ ì†ë„ ë²¤ì¹˜ë§ˆí¬
- â³ Gradient accumulation ê²€ì¦

---

## 8. ë¬¸ì„œí™” ê³„íš

### 8.1 ì‹ ê·œ ìƒì„± ë¬¸ì„œ

#### `docs/model_registry.md`

**ëª©ì **: MLflow Model Registry ì‚¬ìš© ê°€ì´ë“œ

**ë‚´ìš©**:
- Model Registry ê°œìš”
- ëª¨ë¸ ë“±ë¡ ë°©ë²•
- Stage ê´€ë¦¬ (None, Staging, Production, Archived)
- ëª¨ë¸ ë¹„êµ ë° ì„ íƒ
- ë¡¤ë°± ë°©ë²•
- MLflow UI ì‚¬ìš©ë²•
- CLI ì‚¬ìš© ì˜ˆì‹œ

#### `docs/hyperparameter_tuning.md`

**ëª©ì **: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

**ë‚´ìš©**:
- Optuna ê°œìš”
- MLflow í†µí•© ë°©ì‹
- íŠœë‹ ì‹¤í–‰ ë°©ë²• (`make tune`)
- íŠœë‹ íŒŒë¼ë¯¸í„° ì»¤ìŠ¤í„°ë§ˆì´ì§•
- ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
- ìµœì  íŒŒë¼ë¯¸í„° ì ìš© ë°©ë²•
- ì„±ëŠ¥ ê°œì„  íŒ

#### `docs/distributed_training.md`

**ëª©ì **: DDP ë¶„ì‚° í•™ìŠµ ê°€ì´ë“œ (+ TODO)

**ë‚´ìš©**:
- DDP ê°œìš” ë° ìž¥ì 
- ì½”ë“œ êµ¬ì¡° ì„¤ëª…
- ë¡œì»¬ í…ŒìŠ¤íŠ¸ ë°©ë²• (CPU)
- **TODO ì„¹ì…˜**: í´ë¼ìš°ë“œ GPU í…ŒìŠ¤íŠ¸ ê³„íš
  - í•„ìš” í™˜ê²½ (AWS p3.2xlarge ë“±)
  - ì„¤ì • ë°©ë²•
  - ë²¤ì¹˜ë§ˆí¬ ê³„íš
  - ì˜ˆìƒ ë¹„ìš©
- ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 8.2 ì—…ë°ì´íŠ¸ ë¬¸ì„œ

#### `plan.md`

**ì¶”ê°€ ë‚´ìš©**:
- Phase 3 ìš”ì•½ ì„¹ì…˜
- Phase 3.1 ~ 3.5 ì²´í¬ë¦¬ìŠ¤íŠ¸
- `docs/phase3_plan.md` ë§í¬

#### `README.md`

**ì¶”ê°€ ë‚´ìš©**:
- ë¬¸ì„œ ì„¹ì…˜ì— Phase 3 ê³„íš ë§í¬
- TODO.md ë§í¬
- ìƒˆë¡œìš´ ê¸°ëŠ¥ ì†Œê°œ (Model Registry, Optuna)

#### `TESTING.md`

**ì¶”ê°€ ë‚´ìš©** (í•„ìš”ì‹œ):
- í™•ìž¥ í…ŒìŠ¤íŠ¸ ì„¤ëª…
- ì»¤ë²„ë¦¬ì§€ 75% ëª©í‘œ ëª…ì‹œ

---

## 9. ì§„í–‰ ìƒí™© ì¶”ì 

### 9.1 Phase 3.1: í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ê°œì„ 

- [ ] `tests/test_evaluate_extended.py` ìƒì„±
  - [ ] Confusion matrix ìƒì„± í…ŒìŠ¤íŠ¸
  - [ ] Per-class metrics ê³„ì‚° í…ŒìŠ¤íŠ¸
  - [ ] ì‹œê°í™” ì €ìž¥ í…ŒìŠ¤íŠ¸
  - [ ] MLflow ë©”íŠ¸ë¦­ ë¡œê¹… í…ŒìŠ¤íŠ¸
- [ ] `tests/test_dataset_extended.py` ìƒì„±
  - [ ] Train/Val split ë¹„ìœ¨ ê²€ì¦
  - [ ] DataLoader workers í…ŒìŠ¤íŠ¸
  - [ ] Augmentation pipeline í…ŒìŠ¤íŠ¸
  - [ ] ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸
- [ ] `tests/test_training_extended.py` ìƒì„±
  - [ ] Early stopping ë™ìž‘ í…ŒìŠ¤íŠ¸
  - [ ] Checkpoint save/load í…ŒìŠ¤íŠ¸
  - [ ] Learning rate scheduler í…ŒìŠ¤íŠ¸
- [ ] ì»¤ë²„ë¦¬ì§€ 75% ë‹¬ì„± í™•ì¸
- [ ] CI/CD í†µê³¼ í™•ì¸

### 9.2 Phase 3.2: íƒ€ìž… ì•ˆì „ì„± ê°•í™”

- [ ] `src/training/types.py` ìƒì„±
  - [ ] TrainingMetrics TypedDict
  - [ ] EvaluationMetrics TypedDict
  - [ ] OptimizerProtocol
  - [ ] LRSchedulerProtocol
- [ ] ëª¨ë“  í•¨ìˆ˜ íƒ€ìž… ížŒíŠ¸ ì¶”ê°€
  - [ ] `src/training/train.py`
  - [ ] `src/training/evaluate.py`
  - [ ] `src/data/dataset.py`
  - [ ] `src/models/vision_model.py`
- [ ] `pyproject.toml` mypy strict ì„¤ì •
- [ ] `.github/workflows/test.yml` ì—…ë°ì´íŠ¸
- [ ] mypy strict í†µê³¼ í™•ì¸

### 9.3 Phase 3.5: MLflow Model Registry

- [ ] `src/training/registry.py` ìƒì„±
  - [ ] ModelRegistry í´ëž˜ìŠ¤
  - [ ] register_model ë©”ì„œë“œ
  - [ ] promote_model ë©”ì„œë“œ
  - [ ] get_latest_model ë©”ì„œë“œ
  - [ ] compare_models ë©”ì„œë“œ
  - [ ] rollback_to_version ë©”ì„œë“œ
- [ ] `src/training/train.py` í†µí•©
  - [ ] í•™ìŠµ í›„ ìžë™ ë“±ë¡
  - [ ] ì¡°ê±´ë¶€ Staging ìŠ¹ê²©
- [ ] `tests/test_registry.py` ìƒì„±
  - [ ] ëª¨ë¸ ë“±ë¡ í…ŒìŠ¤íŠ¸
  - [ ] Stage ì „í™˜ í…ŒìŠ¤íŠ¸
  - [ ] ë²„ì „ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
- [ ] `docs/model_registry.md` ìž‘ì„±
- [ ] MLflow UIì—ì„œ ë™ìž‘ í™•ì¸

### 9.4 Phase 3.4: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

- [ ] ì˜ì¡´ì„± ì¶”ê°€
  - [ ] `poetry add optuna optuna-integration`
- [ ] `src/training/tuning.py` ìƒì„±
  - [ ] OptunaObjective í´ëž˜ìŠ¤
  - [ ] run_hyperparameter_search í•¨ìˆ˜
  - [ ] MLflow í†µí•©
  - [ ] ì‹œê°í™” ìƒì„±
- [ ] `src/config/settings.py` ìˆ˜ì •
  - [ ] Optuna ì„¤ì • ì¶”ê°€
- [ ] Makefile `tune` ëª…ë ¹ì–´ ì¶”ê°€
- [ ] `tests/test_tuning.py` ìƒì„±
- [ ] `docs/hyperparameter_tuning.md` ìž‘ì„±
- [ ] 50 trials ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸
- [ ] 90%+ ì •í™•ë„ ë‹¬ì„± í™•ì¸

### 9.5 Phase 3.3: DDP ë¶„ì‚° í•™ìŠµ

- [ ] `src/training/distributed.py` ìƒì„±
  - [ ] setup_distributed í•¨ìˆ˜
  - [ ] cleanup_distributed í•¨ìˆ˜
  - [ ] is_main_process í•¨ìˆ˜
  - [ ] get_rank, get_world_size í•¨ìˆ˜
  - [ ] reduce_dict í•¨ìˆ˜
- [ ] `src/training/train_distributed.py` ìƒì„±
- [ ] `src/training/train.py` ìˆ˜ì •
  - [ ] DDP í†µí•©
  - [ ] Main process ë¡œê¹…
- [ ] `src/config/settings.py` ìˆ˜ì •
  - [ ] DDP ì„¤ì • ì¶”ê°€
- [ ] Makefile `train-ddp` ëª…ë ¹ì–´ ì¶”ê°€
- [ ] `tests/test_distributed.py` ìƒì„± (ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë§Œ)
- [ ] `docs/distributed_training.md` ìž‘ì„±
- [ ] `TODO.md` ì—…ë°ì´íŠ¸ (í´ë¼ìš°ë“œ í…ŒìŠ¤íŠ¸ í•­ëª©)
- [ ] ë¡œì»¬ CPU DDP ê¸°ë³¸ ë™ìž‘ í™•ì¸

### 9.6 ë¬¸ì„œí™” ë° ë§ˆë¬´ë¦¬

- [ ] `docs/phase3_plan.md` ìž‘ì„± (ë³¸ ë¬¸ì„œ)
- [ ] `TODO.md` ìž‘ì„±
- [ ] `plan.md` ì—…ë°ì´íŠ¸
- [ ] `README.md` ì—…ë°ì´íŠ¸
- [ ] ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° í†µê³¼ í™•ì¸
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ í†µê³¼ í™•ì¸
- [ ] Phase 3 ì™„ë£Œ ì»¤ë°‹ ë° í‘¸ì‹œ

---

## 10. ì°¸ê³  ìžë£Œ

### 10.1 ê³µì‹ ë¬¸ì„œ

- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Optuna MLflow Integration](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.MLflowCallback.html)
- [PyTorch DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [PyTorch Distributed](https://pytorch.org/docs/stable/distributed.html)
- [mypy Documentation](https://mypy.readthedocs.io/)
- [typing â€” Support for type hints](https://docs.python.org/3/library/typing.html)

### 10.2 ë‚´ë¶€ ë¬¸ì„œ

- [README.md](../README.md): í”„ë¡œì íŠ¸ ê°œìš”
- [plan.md](../plan.md): ì „ì²´ Phase ê³„íš
- [TESTING.md](../TESTING.md): í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ
- [CICD.md](../CICD.md): CI/CD íŒŒì´í”„ë¼ì¸ ê°€ì´ë“œ
- [TODO.md](../TODO.md): ì¶”í›„ ìž‘ì—… ëª©ë¡

### 10.3 ê´€ë ¨ ì´ìŠˆ ë° ì°¸ê³  ìžë£Œ

- [PyTorch MPS Backend](https://pytorch.org/docs/stable/notes/mps.html)
- [DDP on Apple Silicon](https://github.com/pytorch/pytorch/issues/77764): M2ì—ì„œ DDP ì œì•½
- [Optuna Pruners](https://optuna.readthedocs.io/en/stable/reference/pruners.html): Early stopping for trials
- [MLflow Autologging](https://mlflow.org/docs/latest/tracking/autolog.html)

---

## ìš”ì•½

Phase 3ì€ **í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆê³¼ ê¸°ëŠ¥ì„ í•œ ë‹¨ê³„ ëŒì–´ì˜¬ë¦¬ëŠ”** ìž‘ì—…ìž…ë‹ˆë‹¤.

**í•µì‹¬ ëª©í‘œ**:
1. âœ… í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 75%+ (ì‹ ë¢°ì„± í™•ë³´)
2. âœ… mypy strict ëª¨ë“œ (íƒ€ìž… ì•ˆì „ì„± ê°•í™”)
3. âœ… MLflow Model Registry (ëª¨ë¸ ìƒëª…ì£¼ê¸° ìžë™í™”)
4. âœ… Optuna íŠœë‹ (90%+ ì •í™•ë„ ë‹¬ì„±)
5. âœ… DDP ì½”ë“œ êµ¬ì¡° (í™•ìž¥ì„± ì¤€ë¹„)

**ì˜ˆìƒ ê¸°ê°„**: 3-4ì£¼

**ì œì•½ì‚¬í•­**: M2 Macì—ì„œ DDP multi-GPU í…ŒìŠ¤íŠ¸ ë¶ˆê°€ â†’ ì½”ë“œë§Œ ì™„ì„±, ì‹¤ì œ í…ŒìŠ¤íŠ¸ëŠ” í´ë¼ìš°ë“œì—ì„œ (TODO.md)

**ë‹¤ìŒ ë‹¨ê³„**: Phase 3 ì™„ë£Œ í›„ Phase 5 (ëª¨ë¸ ê°œì„ ) ë˜ëŠ” Phase 7 (Kubernetes ë°°í¬)

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-18
**ìž‘ì„±ìž**: MLflow Vision Training System Team

# TODO - ì¶”í›„ ì‘ì—… ëª©ë¡

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-24
**ë²„ì „**: 2.1

ì´ ë¬¸ì„œëŠ” í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œ ì™„ì „íˆ êµ¬í˜„ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ì¶”í›„ í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì‘ì—…ë“¤ì„ ì •ë¦¬í•œ ëª©ë¡ì…ë‹ˆë‹¤.

## ìµœê·¼ ì™„ë£Œ í•­ëª© (Phase 4.6)

### âœ… Ray Tune í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (2025-10-24)

**ì™„ë£Œ í•­ëª©**:
- âœ… Ray Tune ì½”ì–´ ëª¨ë“ˆ êµ¬í˜„ (`src/tuning/ray_tune.py`)
- âœ… MLflow ì™„ì „ í†µí•© (ë°°ì¹˜ ë ˆë²¨ + ì—í¬í¬ ë ˆë²¨ ë©”íŠ¸ë¦­)
- âœ… ASHA ìŠ¤ì¼€ì¤„ëŸ¬ ë° HyperOpt ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
- âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰ ê³µê°„ ì •ì˜ (learning_rate, weight_decay, momentum)
- âœ… Makefile ëª…ë ¹ì–´ (`make tune`, `make tune-quick`, `make tune-extensive`)
- âœ… í†µí•© í…ŒìŠ¤íŠ¸ (`test_ray_tune.py`)
- âœ… ë¬¸ì„œí™” (plan.md, README.md, TODO.md)

**ì£¼ìš” ê¸°ëŠ¥**:
- ë¶„ì‚° í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ (Ray Tune)
- MLflow ìë™ ë¡œê¹… (batch-level + epoch-level ë©”íŠ¸ë¦­)
- Best trial ìë™ ì„ íƒ ë° ê¸°ë¡
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° ë³µêµ¬

**í–¥í›„ ê³„íš**:
- Phase 5ì—ì„œ EKS + Ray Clusterë¡œ í™•ì¥
- GPU ì›Œì»¤ ë…¸ë“œë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ íŠœë‹
- Ray Trainê³¼ í†µí•©í•˜ì—¬ DDP ë¶„ì‚° í•™ìŠµ ì§€ì›

---

## ğŸ“Œ í”„ë¡œì íŠ¸ ë°©í–¥ ë³€ê²½ (2025-10-21)

**ìƒˆë¡œìš´ ìš°ì„ ìˆœìœ„**: Phase 5-7 (EKS ê¸°ë°˜ MLOps í”Œë«í¼) êµ¬ì¶•ì´ ìµœìš°ì„  ê³¼ì œë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

ê¸°ì¡´ì˜ ë¡œì»¬ í™˜ê²½ ìµœì í™”(Phase 3.3 DDP í…ŒìŠ¤íŠ¸, ëª¨ë¸ ìµœì í™” ë“±)ëŠ” EKS ì¸í”„ë¼ êµ¬ì¶• ì´í›„ë¡œ ì—°ê¸°ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## ëª©ì°¨

### í˜„ì¬ ìš°ì„ ìˆœìœ„ (Phase 5-7)
1. [Phase 5: AWS EKS ì¸í”„ë¼ êµ¬ì¶•](#phase-5-aws-eks-ì¸í”„ë¼-êµ¬ì¶•)
2. [Phase 6: Ray Tune í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”](#phase-6-ray-tune-í•˜ì´í¼íŒŒë¼ë¯¸í„°-ìµœì í™”)
3. [Phase 7: DDP ë¶„ì‚° í•™ìŠµ + Airflow](#phase-7-ddp-ë¶„ì‚°-í•™ìŠµ--airflow)

### ì¶”í›„ ì‘ì—… (Phase 5-7 ì´í›„)
4. [ëª¨ë¸ ìµœì í™”](#ëª¨ë¸-ìµœì í™”)
5. [ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œì„ ](#ë°ì´í„°-íŒŒì´í”„ë¼ì¸-ê°œì„ )
6. [MLflow ê³ ê¸‰ ê¸°ëŠ¥](#mlflow-ê³ ê¸‰-ê¸°ëŠ¥)
7. [CI/CD ê°œì„ ](#cicd-ê°œì„ )
8. [ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±](#ëª¨ë‹ˆí„°ë§-ë°-ê´€ì°°ì„±)

---

## Phase 5: AWS EKS ì¸í”„ë¼ êµ¬ì¶•

**ìš°ì„ ìˆœìœ„**: Critical (ìµœìš°ì„ )
**ì˜ˆìƒ ê¸°ê°„**: 4-5ì£¼
**ì˜ˆìƒ ë¹„ìš©**: ~$190/ì›” (ê¸°ë³¸ ìš´ì˜)

**ëª©í‘œ**: ì¤‘ì•™í™”ëœ MLflow ì„œë²„ë¥¼ EKSì— ë°°í¬í•˜ì—¬ ë©€í‹° ìœ ì € í™˜ê²½ êµ¬ì¶•

**ìƒì„¸ ë¬¸ì„œ**: [docs/eks_infrastructure.md](docs/eks_infrastructure.md) (ì‘ì„± ì˜ˆì •)

---

### 5.1 AWS ê¸°ë³¸ ì¸í”„ë¼ (1ì£¼)

**ìš°ì„ ìˆœìœ„**: Critical

#### Terraform í”„ë¡œì íŠ¸ êµ¬ì¡°
- [ ] `terraform/aws-eks/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] `terraform/aws-eks/main.tf` - EKS í´ëŸ¬ìŠ¤í„° ì •ì˜
  - [ ] VPC ì„¤ì • (3 AZ, Public/Private ì„œë¸Œë„·)
  - [ ] EKS í´ëŸ¬ìŠ¤í„° (Kubernetes 1.28+)
  - [ ] Node Group (t3.medium Ã— 2, Auto-scaling)
- [ ] `terraform/aws-eks/rds.tf` - PostgreSQL ë°ì´í„°ë² ì´ìŠ¤
  - [ ] db.t3.small (2 vCPU, 2GB RAM)
  - [ ] Multi-AZ ë°°í¬
  - [ ] ì•”í˜¸í™” í™œì„±í™”
- [ ] `terraform/aws-eks/s3.tf` - S3 ë²„í‚·
  - [ ] Versioning í™œì„±í™”
  - [ ] Lifecycle policy (90ì¼ í›„ Glacier)
- [ ] `terraform/aws-eks/iam.tf` - IRSA ê¶Œí•œ
  - [ ] MLflow Pod â†’ S3 ì ‘ê·¼ ê¶Œí•œ
  - [ ] Ray Cluster â†’ S3 ì ‘ê·¼ ê¶Œí•œ
- [ ] `terraform/aws-eks/variables.tf` - ë³€ìˆ˜ ì •ì˜
- [ ] `terraform/aws-eks/outputs.tf` - ì¶œë ¥ ì •ì˜

#### ë°°í¬ ë° ê²€ì¦
- [ ] `terraform init` ì‹¤í–‰
- [ ] `terraform plan` ê²€í† 
- [ ] `terraform apply` ì‹¤í–‰
- [ ] `aws eks update-kubeconfig --name mdpg-mlops` ì‹¤í–‰
- [ ] `kubectl get nodes` í™•ì¸

**ì˜ˆìƒ ë¹„ìš©**:
- EKS Control Plane: $73/ì›”
- Worker Nodes (t3.medium Ã— 2): $60/ì›”
- RDS (db.t3.small): $30/ì›”
- S3: ~$5/ì›”
- **ì´ê³„**: ~$168/ì›”

---

### 5.2 MLflow ì„œë²„ ë°°í¬ (1ì£¼)

**ìš°ì„ ìˆœìœ„**: Critical

#### Helm Chart ì‘ì„±
- [ ] `charts/mlflow/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] `charts/mlflow/Chart.yaml`
- [ ] `charts/mlflow/values.yaml` - ì„¤ì • ì •ì˜
  ```yaml
  replicaCount: 2
  image:
    repository: ghcr.io/[username]/mlflow-study-mlflow
    tag: latest

  env:
    BACKEND_STORE_URI: postgresql://...
    ARTIFACT_ROOT: s3://mlflow-artifacts/

  ingress:
    enabled: true
    className: alb
    hosts:
      - host: mlflow.mdpg.ai

  hpa:
    minReplicas: 2
    maxReplicas: 5
  ```

- [ ] `charts/mlflow/templates/deployment.yaml`
- [ ] `charts/mlflow/templates/service.yaml`
- [ ] `charts/mlflow/templates/ingress.yaml` (ALB Ingress Controller)
- [ ] `charts/mlflow/templates/hpa.yaml` (Horizontal Pod Autoscaler)
- [ ] `charts/mlflow/templates/secret.yaml` (DB ìê²©ì¦ëª…)

#### MLflow Authentication ì„¤ì •
- [ ] MLflow Authentication í™œì„±í™”
  ```bash
  mlflow server \
    --backend-store-uri postgresql://... \
    --default-artifact-root s3://... \
    --app-name basic-auth
  ```
- [ ] ì‚¬ìš©ì ê³„ì • ìƒì„± (MLOps Ã— 2, ML Engineer Ã— 1)
- [ ] ê¶Œí•œ ì„¤ì • (READ/EDIT/MANAGE)
- [ ] Kubernetes Secretìœ¼ë¡œ ìê²©ì¦ëª… ì €ì¥

#### SSL/TLS ì„¤ì •
- [ ] AWS Certificate Managerì—ì„œ ì¸ì¦ì„œ ë°œê¸‰
- [ ] ALB Ingressì— HTTPS ì ìš©
- [ ] HTTP â†’ HTTPS ë¦¬ë‹¤ì´ë ‰íŠ¸ ì„¤ì •

#### ë°°í¬ ë° ê²€ì¦
- [ ] `helm install mlflow ./charts/mlflow`
- [ ] `kubectl get pods -n ml-platform` í™•ì¸
- [ ] `kubectl logs -f deployment/mlflow` í™•ì¸
- [ ] MLflow UI ì ‘ì† (https://mlflow.mdpg.ai)
- [ ] ì‚¬ìš©ì ë¡œê·¸ì¸ í…ŒìŠ¤íŠ¸

---

### 5.3 í´ë¼ì´ì–¸íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜ (1ì¼)

**ìš°ì„ ìˆœìœ„**: High

#### VSCode í™˜ê²½ ì„¤ì • ê°€ì´ë“œ
- [ ] `.env.remote` í…œí”Œë¦¿ ìƒì„±
  ```bash
  # Remote MLflow Server
  MLFLOW_TRACKING_URI=https://mlflow.mdpg.ai
  MLFLOW_TRACKING_USERNAME=ml_engineer_1
  MLFLOW_TRACKING_PASSWORD=***

  # S3 Configuration (IRSAë¡œ ìë™ ì²˜ë¦¬)
  # AWS_REGION=us-west-2
  ```

- [ ] VSCode settings.json ì˜ˆì œ ì‘ì„±
- [ ] Python í™˜ê²½ ì„¤ì • ê°€ì´ë“œ

#### ë¡œì»¬ â†’ ì›ê²© ì „í™˜ í…ŒìŠ¤íŠ¸
- [ ] ê¸°ì¡´ í•™ìŠµ ì½”ë“œ ìˆ˜ì • ì—†ì´ ì‹¤í—˜ ì‹¤í–‰
  ```bash
  cp .env.remote .env
  poetry run python -m src.training.train
  ```
- [ ] MLflow UIì—ì„œ ì‹¤í—˜ ê²°ê³¼ í™•ì¸
- [ ] ì•„í‹°íŒ©íŠ¸ê°€ S3ì— ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
  ```bash
  aws s3 ls s3://mlflow-artifacts/
  ```

**ìƒì„¸ ê°€ì´ë“œ**: [docs/vscode_setup.md](docs/vscode_setup.md) (ì‘ì„± ì˜ˆì •)

---

### 5.4 ë°°í¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (2-3ì¼)

**ìš°ì„ ìˆœìœ„**: High (íœ´ë¨¼ ì—ëŸ¬ ìµœì†Œí™”)

#### Setup Scripts
- [ ] `scripts/setup/01-setup-aws.sh`
  ```bash
  #!/bin/bash
  set -e

  echo "ğŸ”§ AWS CLI ì„¤ì • í™•ì¸"
  aws sts get-caller-identity

  echo "âœ… AWS ì„¤ì • ì™„ë£Œ"
  ```

- [ ] `scripts/setup/02-deploy-eks.sh`
  ```bash
  #!/bin/bash
  set -e

  echo "ğŸš€ EKS í´ëŸ¬ìŠ¤í„° ë°°í¬"
  cd terraform/aws-eks
  terraform init
  terraform plan -out=tfplan

  read -p "Deploy? (yes/no): " confirm
  [[ "$confirm" != "yes" ]] && exit 0

  terraform apply tfplan

  echo "ğŸ“ kubeconfig ì—…ë°ì´íŠ¸"
  aws eks update-kubeconfig --name mdpg-mlops

  echo "âœ… EKS ë°°í¬ ì™„ë£Œ"
  kubectl get nodes
  ```

- [ ] `scripts/setup/03-deploy-mlflow.sh`
  ```bash
  #!/bin/bash
  set -e

  echo "ğŸš€ MLflow ì„œë²„ ë°°í¬"
  helm install mlflow ./charts/mlflow \
    --namespace ml-platform \
    --create-namespace \
    --values charts/mlflow/values-production.yaml

  echo "â³ Pod ì‹œì‘ ëŒ€ê¸°"
  kubectl wait --for=condition=ready pod -l app=mlflow -n ml-platform --timeout=300s

  echo "âœ… MLflow ë°°í¬ ì™„ë£Œ"
  kubectl get pods -n ml-platform
  ```

- [ ] `scripts/setup/04-setup-users.sh`
  ```bash
  #!/bin/bash
  set -e

  echo "ğŸ‘¥ MLflow ì‚¬ìš©ì ê³„ì • ìƒì„±"
  kubectl exec -it deployment/mlflow -n ml-platform -- \
    mlflow server create-user \
      --username mlops_engineer_1 \
      --password [SECURE_PASSWORD]

  # ì¶”ê°€ ì‚¬ìš©ì...
  echo "âœ… ì‚¬ìš©ì ê³„ì • ìƒì„± ì™„ë£Œ"
  ```

- [ ] `scripts/setup/05-test-connection.sh`
  ```bash
  #!/bin/bash
  set -e

  echo "ğŸ” ì—°ê²° í…ŒìŠ¤íŠ¸"
  export MLFLOW_TRACKING_URI=https://mlflow.mdpg.ai
  export MLFLOW_TRACKING_USERNAME=mlops_engineer_1

  python -c "
  import mlflow
  mlflow.set_experiment('connection-test')
  with mlflow.start_run():
      mlflow.log_param('test', 'success')
  print('âœ… MLflow ì—°ê²° ì„±ê³µ')
  "
  ```

- [ ] `scripts/setup/06-verify-all.sh`
  ```bash
  #!/bin/bash
  set -e

  echo "ğŸ” ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦"

  # EKS í™•ì¸
  kubectl get nodes

  # MLflow í™•ì¸
  kubectl get pods -n ml-platform

  # RDS í™•ì¸
  kubectl exec -it deployment/mlflow -n ml-platform -- \
    psql $BACKEND_STORE_URI -c "SELECT version();"

  # S3 í™•ì¸
  aws s3 ls s3://mlflow-artifacts/

  echo "âœ… ì „ì²´ ê²€ì¦ ì™„ë£Œ"
  ```

#### Operations Scripts
- [ ] `scripts/ops/backup-mlflow.sh` - PostgreSQL ë°±ì—…
- [ ] `scripts/ops/restore-mlflow.sh` - ë³µì›
- [ ] `scripts/ops/scale-mlflow.sh` - ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§
- [ ] `scripts/ops/logs-mlflow.sh` - ë¡œê·¸ ìˆ˜ì§‘

**ìƒì„¸ ê°€ì´ë“œ**: [docs/deployment_scripts.md](docs/deployment_scripts.md) (ì‘ì„± ì˜ˆì •)

---

### Phase 5 ì„±ê³µ ê¸°ì¤€

- [ ] âœ… MLflow ì„œë²„ê°€ EKSì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰
- [ ] âœ… RDS PostgreSQL ì—°ê²° ì„±ê³µ
- [ ] âœ… S3 ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì„±ê³µ
- [ ] âœ… MLflow Authentication ë™ì‘ (ìµœì†Œ 3ëª… ì‚¬ìš©ì)
- [ ] âœ… HTTPS ì ‘ì† ê°€ëŠ¥ (https://mlflow.mdpg.ai)
- [ ] âœ… ëª¨ë“  ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì •ìƒ ì‘ë™
- [ ] âœ… ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ ì›ê²© MLflow ì„œë²„ ì ‘ì† ì„±ê³µ
- [ ] âœ… HPA (Horizontal Pod Autoscaler) ë™ì‘ í™•ì¸

---

## Phase 6: Ray Cluster ë¶„ì‚° í•™ìŠµ í™•ì¥

**ìš°ì„ ìˆœìœ„**: High
**ì˜ˆìƒ ê¸°ê°„**: 2-3ì£¼
**ì„ í–‰ ì¡°ê±´**: Phase 5 ì™„ë£Œ
**í˜„ì¬ ìƒíƒœ**: Phase 4.6ì—ì„œ ë¡œì»¬ Ray Tune ì™„ë£Œ âœ…

**ëª©í‘œ**: EKS + Ray Clusterì—ì„œ ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ë¶„ì‚° í•™ìŠµ

**ìƒì„¸ ë¬¸ì„œ**: [docs/ray_tune_guide.md](docs/ray_tune_guide.md) (ì‘ì„± ì˜ˆì •)

**ì°¸ê³ **:
- Ray Tune ì½”ì–´ ê¸°ëŠ¥ì€ Phase 4.6ì—ì„œ ì™„ë£Œë¨
- Phase 6ëŠ” EKS í™˜ê²½ì—ì„œ GPU ì›Œì»¤ ë…¸ë“œë¥¼ í™œìš©í•œ í™•ì¥ì— ì§‘ì¤‘

---

### 6.1 Ray Cluster ë°°í¬ (1ì£¼)

**ìš°ì„ ìˆœìœ„**: High

#### KubeRay Operator ì„¤ì¹˜
- [ ] KubeRay Operator YAML ë‹¤ìš´ë¡œë“œ
  ```bash
  kubectl create -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/default/operator.yaml
  ```
- [ ] Operator ì •ìƒ ë™ì‘ í™•ì¸
  ```bash
  kubectl get pods -n ray-system
  ```

#### Ray Cluster Helm Chart
- [ ] `charts/ray-cluster/Chart.yaml`
- [ ] `charts/ray-cluster/values.yaml`
  ```yaml
  head:
    cpu: 2
    memory: 4Gi
    replicas: 1

  worker:
    minReplicas: 0
    maxReplicas: 5
    cpu: 4
    memory: 16Gi
    gpu: 1
    nodeType: p3.2xlarge  # Spot Instance
  ```
- [ ] `charts/ray-cluster/templates/raycluster.yaml`

#### GPU ë…¸ë“œ ê·¸ë£¹ ì¶”ê°€
- [ ] Terraformì— GPU ë…¸ë“œ ê·¸ë£¹ ì¶”ê°€
  ```hcl
  resource "aws_eks_node_group" "gpu_nodes" {
    node_group_name = "gpu-workers"
    instance_types  = ["p3.2xlarge"]
    capacity_type   = "SPOT"  # ë¹„ìš© ì ˆê°

    scaling_config {
      min_size     = 0
      desired_size = 0
      max_size     = 5
    }
  }
  ```
- [ ] `terraform apply` ì‹¤í–‰

#### Ray Dashboard ì ‘ê·¼
- [ ] Port-forward ì„¤ì •
  ```bash
  kubectl port-forward svc/ray-head 8265:8265 -n ray-system
  ```
- [ ] ë˜ëŠ” Ingress ì„¤ì • (ì„ íƒ)

---

### 6.2 Ray Tune ì½”ë“œ ì‘ì„± (1ì£¼)

**ìš°ì„ ìˆœìœ„**: High

#### Ray Tune í†µí•©
- [ ] `src/tuning/__init__.py`
- [ ] `src/tuning/ray_tune.py`
  ```python
  from ray import tune
  from ray.tune.integration.mlflow import MLflowLoggerCallback

  def train_cifar10(config):
      # PyTorch í•™ìŠµ ì½”ë“œ
      model = VisionModel(
          model_name=config["model_name"],
          num_classes=10
      )

      # í•™ìŠµ ë£¨í”„
      for epoch in range(config["epochs"]):
          train_loss = train_epoch(model, ...)
          val_acc = validate(model, ...)

          # Ray Tuneì— ë©”íŠ¸ë¦­ ë³´ê³ 
          tune.report(val_accuracy=val_acc, train_loss=train_loss)

  # íƒìƒ‰ ê³µê°„ ì •ì˜
  config = {
      "model_name": tune.choice(["mobilenet_v3_small", "mobilenet_v3_large", "resnet18"]),
      "learning_rate": tune.loguniform(1e-4, 1e-1),
      "batch_size": tune.choice([32, 64, 128]),
      "epochs": 20,
  }

  # Ray Tune ì‹¤í–‰
  analysis = tune.run(
      train_cifar10,
      config=config,
      num_samples=100,  # 100 trials
      resources_per_trial={"gpu": 1},
      callbacks=[MLflowLoggerCallback(
          tracking_uri="https://mlflow.mdpg.ai",
          experiment_name="ray-tune-cifar10"
      )]
  )
  ```

#### Search Algorithms
- [ ] `src/tuning/search_algorithms.py`
  ```python
  from ray.tune.schedulers import ASHAScheduler, HyperBandScheduler
  from ray.tune.search.optuna import OptunaSearch

  # ASHA (Asynchronous Successive Halving)
  scheduler = ASHAScheduler(
      time_attr='training_iteration',
      metric='val_accuracy',
      mode='max',
      max_t=20,
      grace_period=5,
      reduction_factor=3
  )

  # Optuna Bayesian Optimization
  search_alg = OptunaSearch(
      metric="val_accuracy",
      mode="max"
  )
  ```

#### GPU ìŠ¤ì¼€ì¤„ë§
- [ ] `resources_per_trial={"gpu": 1}` ì„¤ì •
- [ ] Fractional GPU ì‹¤í—˜ (í•„ìš”ì‹œ)
  ```python
  resources_per_trial={"gpu": 0.5}  # 1 GPUì— 2 trials
  ```

---

### 6.3 ì‹¤í—˜ ì‹¤í–‰ (3-5ì¼)

**ìš°ì„ ìˆœìœ„**: High

#### 100 Trials í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
- [ ] Ray Tune ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
  ```bash
  poetry run python -m src.tuning.ray_tune --num-samples 100
  ```
- [ ] Ray Dashboardì—ì„œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
- [ ] MLflow UIì—ì„œ ì‹¤í—˜ ê²°ê³¼ í™•ì¸

#### ëª©í‘œ
- [ ] Test Accuracy 90%+ ë‹¬ì„±
- [ ] ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ë°œê²¬
- [ ] MLflowì— ëª¨ë“  trial ìë™ ê¸°ë¡

#### ìµœì  ëª¨ë¸ ë“±ë¡
- [ ] MLflow Model Registryì— Best Model ë“±ë¡
  ```python
  best_model_uri = analysis.best_checkpoint
  mlflow.register_model(best_model_uri, "vision-classifier")
  ```
- [ ] Production ìŠ¤í…Œì´ì§€ë¡œ ì „í™˜

---

### Phase 6 ì„±ê³µ ê¸°ì¤€

- [ ] âœ… Ray Cluster EKSì—ì„œ ì•ˆì • ì‹¤í–‰
- [ ] âœ… GPU auto-scaling ë™ì‘ (0 â†’ 5 â†’ 0)
- [ ] âœ… 100 trials ì™„ë£Œ
- [ ] âœ… CIFAR-10 ì •í™•ë„ 90%+ ë‹¬ì„±
- [ ] âœ… MLflowì— ëª¨ë“  ì‹¤í—˜ ìë™ ê¸°ë¡
- [ ] âœ… GPU ë¹„ìš© $20 ì´í•˜ (Spot Instance í™œìš©)

---

## Phase 7: DDP ë¶„ì‚° í•™ìŠµ + Airflow

**ìš°ì„ ìˆœìœ„**: Medium
**ì˜ˆìƒ ê¸°ê°„**: 2-3ì£¼
**ì„ í–‰ ì¡°ê±´**: Phase 5, 6 ì™„ë£Œ

**ëª©í‘œ**: PyTorch DDP ë©€í‹° GPU í•™ìŠµ ë° Airflow íŒŒì´í”„ë¼ì¸ ìë™í™”

**ìƒì„¸ ë¬¸ì„œ**: [docs/distributed_training.md](docs/distributed_training.md) (ì‘ì„± ì˜ˆì •)

---

### 7.1 PyTorch DDP êµ¬í˜„ (1ì£¼)

**ìš°ì„ ìˆœìœ„**: Medium

#### DDP ìœ í‹¸ë¦¬í‹°
- [ ] `src/training/distributed.py`
  ```python
  import torch.distributed as dist

  def setup_distributed(rank, world_size, backend='nccl'):
      dist.init_process_group(backend, rank=rank, world_size=world_size)

  def cleanup_distributed():
      dist.destroy_process_group()
  ```

#### DDP í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- [ ] `src/training/train_ddp.py`
  ```python
  from torch.nn.parallel import DistributedDataParallel as DDP

  def main(rank, world_size):
      setup_distributed(rank, world_size)

      model = VisionModel(...).to(rank)
      model = DDP(model, device_ids=[rank])

      # í•™ìŠµ ë£¨í”„
      for epoch in range(epochs):
          train_epoch(model, ...)

          # Rank 0ë§Œ MLflow ë¡œê¹…
          if rank == 0:
              mlflow.log_metrics(...)

      cleanup_distributed()
  ```

#### Docker ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸
- [ ] Dockerfileì— NCCL ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
- [ ] SSH ì„¤ì • (ë©€í‹° ë…¸ë“œ DDP)

---

### 7.2 DDP í…ŒìŠ¤íŠ¸ (3-5ì¼)

**ìš°ì„ ìˆœìœ„**: Medium

#### Ray Trainìœ¼ë¡œ DDP ì‹¤í–‰
- [ ] Single-node multi-GPU (p3.8xlarge, 4 GPUs)
  ```python
  from ray.train.torch import TorchTrainer

  trainer = TorchTrainer(
      train_func=train_ddp,
      scaling_config=ScalingConfig(num_workers=4, use_gpu=True)
  )
  trainer.fit()
  ```

- [ ] Multi-node multi-GPU (p3.2xlarge Ã— 4)
  ```python
  scaling_config=ScalingConfig(num_workers=4, resources_per_worker={"GPU": 1})
  ```

#### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] í•™ìŠµ ì‹œê°„ ë¹„êµí‘œ ì‘ì„±

  | ì„¤ì • | í•™ìŠµ ì‹œê°„ (20 epochs) | Speedup |
  |------|---------------------|---------|
  | 1 GPU | ??? | 1.0x |
  | 4 GPUs (DDP) | ??? | ???x |

- [ ] ëª©í‘œ: 4 GPUsë¡œ 3x ì´ìƒ speedup

---

### 7.3 Airflow íŒŒì´í”„ë¼ì¸ (1ì£¼)

**ìš°ì„ ìˆœìœ„**: Medium

#### Airflow Helm Chart ë°°í¬
- [ ] `charts/airflow/values.yaml`
  ```yaml
  executor: KubernetesExecutor
  dags:
    gitSync:
      enabled: true
      repo: https://github.com/[username]/mlflow-study.git
      branch: main
      subPath: dags/
  ```
- [ ] `helm install airflow apache-airflow/airflow`

#### DAG ì‘ì„±
- [ ] `dags/daily_training.py`
  ```python
  from airflow import DAG
  from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator

  with DAG('daily_training', schedule_interval='@daily') as dag:
      train_task = KubernetesPodOperator(
          task_id='train_model',
          image='ghcr.io/[username]/mlflow-study:latest',
          cmds=['poetry', 'run', 'python', '-m', 'src.training.train'],
          env_vars={
              'MLFLOW_TRACKING_URI': 'https://mlflow.mdpg.ai',
              'EXPERIMENT_NAME': 'daily-training',
              'EPOCHS': '20'
          }
      )
  ```

- [ ] `dags/hyperparameter_tuning.py` - ì£¼ê°„ Ray Tune ì‹¤í–‰
- [ ] `dags/model_evaluation.py` - ëª¨ë¸ í‰ê°€

#### MLflow í†µí•©
- [ ] ëª¨ë“  DAGì—ì„œ MLflow ìë™ ë¡œê¹…
- [ ] ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ë“±ë¡

---

### Phase 7 ì„±ê³µ ê¸°ì¤€

- [ ] âœ… DDP ë©€í‹° GPU í•™ìŠµ ì„±ê³µ
- [ ] âœ… 4 GPUsë¡œ í•™ìŠµ ì‹œê°„ 3x ë‹¨ì¶•
- [ ] âœ… Airflow DAG ìë™ ì‹¤í–‰ (ì¼ì¼ ìŠ¤ì¼€ì¤„)
- [ ] âœ… MLflowì— ëª¨ë“  íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ê¸°ë¡
- [ ] âœ… ë‹¤ì¤‘ ì‹¤í—˜ ë™ì‹œ ì‹¤í–‰ (5ê°œ+)

---

## ì¶”í›„ ì‘ì—… (Phase 5-7 ì´í›„)

ì•„ë˜ ì‘ì—…ë“¤ì€ EKS ì¸í”„ë¼ êµ¬ì¶• ë° Ray Tune, DDP êµ¬í˜„ ì´í›„ì— ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

## Phase 3.3 DDP í…ŒìŠ¤íŠ¸ (í´ë¼ìš°ë“œ í•„ìš”)

### ë°°ê²½ ë° ì œì•½ì‚¬í•­

**í˜„ì¬ ìƒí™©**:
- Phase 3.3ì—ì„œ DDP (Distributed Data Parallel) **ì½”ë“œ êµ¬ì¡° ì™„ì„±**
- `src/training/distributed.py`, `src/training/train_distributed.py` êµ¬í˜„ ì™„ë£Œ
- ë¡œì»¬ CPU 2-process ê¸°ë³¸ ë™ì‘ ê²€ì¦ ì™„ë£Œ

**ì œì•½ì‚¬í•­**:
- âŒ MacBook M2 MPS backendëŠ” PyTorch DDP ë¯¸ì§€ì›
- âŒ ë¡œì»¬ CPU DDPëŠ” í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ëŠë¦¼ (ì‹¤ìš©ì„± ì—†ìŒ)
- âŒ Multi-GPU í™˜ê²½ ì—†ìŒ (ì‹¤ì œ ë¶„ì‚° í•™ìŠµ í…ŒìŠ¤íŠ¸ ë¶ˆê°€)

**ê²°ë¡ **:
- âœ… DDP ì½”ë“œ êµ¬ì¡°ëŠ” ì™„ì„±
- â³ **ì‹¤ì œ multi-GPU í…ŒìŠ¤íŠ¸ëŠ” í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ìˆ˜í–‰ í•„ìš”**

---

### í•„ìš” í™˜ê²½

#### Option 1: AWS EC2 (ì¶”ì²œ)
```
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: p3.2xlarge
- GPU: 1x NVIDIA V100 (16GB)
- vCPU: 8
- RAM: 61 GB
- ë¹„ìš©: $3.06/hour (ì˜¨ë””ë§¨ë“œ)
- ë¹„ìš©: ~$0.90/hour (Spot Instance, ì•½ 70% í• ì¸)

ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: p3.8xlarge (4-GPU í…ŒìŠ¤íŠ¸ìš©)
- GPU: 4x NVIDIA V100 (16GB each)
- vCPU: 32
- RAM: 244 GB
- ë¹„ìš©: $12.24/hour (ì˜¨ë””ë§¨ë“œ)
- ë¹„ìš©: ~$3.60/hour (Spot Instance)
```

#### Option 2: GCP Compute Engine
```
ë¨¸ì‹  íƒ€ì…: n1-standard-8 + 2x T4
- GPU: 2x NVIDIA T4 (16GB each)
- vCPU: 8
- RAM: 30 GB
- ë¹„ìš©: ~$2.50/hour
```

#### Option 3: Google Colab Pro+ (ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš©)
```
- GPU: A100 or V100 (ê°€ë³€)
- ë¹„ìš©: $49.99/month (ë¬´ì œí•œ ì‹¤í–‰ ì‹œê°„)
- ì œì•½: ë‹¨ì¼ GPUë§Œ ì§€ì›, DDP í…ŒìŠ¤íŠ¸ ì œí•œì 
```

**ì¶”ì²œ**: AWS p3.2xlarge Spot Instance (ë¹„ìš© íš¨ìœ¨ì )

---

### í™˜ê²½ ì„¤ì • ê°€ì´ë“œ

#### 1. AWS EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

```bash
# AWS CLIë¡œ Spot Instance ìš”ì²­
aws ec2 request-spot-instances \
  --instance-count 1 \
  --type "one-time" \
  --launch-specification \
    InstanceType=p3.2xlarge,\
    ImageId=ami-0c55b159cbfafe1f0,\  # Deep Learning AMI
    KeyName=my-key-pair,\
    SecurityGroupIds=sg-xxxxxx
```

#### 2. CUDA ë° PyTorch ì„¤ì¹˜

```bash
# Deep Learning AMI ì‚¬ìš© ì‹œ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŒ
# í™•ì¸
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# ìˆ˜ë™ ì„¤ì¹˜ê°€ í•„ìš”í•œ ê²½ìš°
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia
```

#### 3. í”„ë¡œì íŠ¸ ì½”ë“œ ë°°í¬

```bash
# Git clone
git clone https://github.com/[username]/mlflow-study.git
cd mlflow-study

# Poetry ì„¤ì¹˜
curl -sSL https://install.python-poetry.org | python3 -

# ì˜ì¡´ì„± ì„¤ì¹˜
poetry install

# MLflow ì¸í”„ë¼ ì‹œì‘ (Docker Compose)
make up
```

---

### ì‘ì—… ëª©ë¡

#### 1. Multi-GPU í•™ìŠµ í…ŒìŠ¤íŠ¸

**ìš°ì„ ìˆœìœ„**: High
**ì˜ˆìƒ ì†Œìš”**: 4-6ì‹œê°„
**ì˜ˆìƒ ë¹„ìš©**: ~$5-10 (Spot Instance ì‚¬ìš© ì‹œ)

- [ ] **Single GPU ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •**
  ```bash
  # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
  export DISTRIBUTED=false
  export EPOCHS=10
  export DATASET=CIFAR10

  # í•™ìŠµ ì‹¤í–‰
  poetry run python -m src.training.train

  # ê¸°ë¡í•  ë©”íŠ¸ë¦­:
  # - í•™ìŠµ ì‹œê°„ (ì´ˆ)
  # - Throughput (images/sec)
  # - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
  # - ìµœì¢… ì •í™•ë„
  ```

- [ ] **2-GPU DDP í…ŒìŠ¤íŠ¸ (p3.2xlarge 2ëŒ€ ë˜ëŠ” p3.8xlarge 1ëŒ€)**
  ```bash
  # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
  export DISTRIBUTED=true
  export BACKEND=nccl
  export MASTER_ADDR=localhost
  export MASTER_PORT=12355

  # DDP í•™ìŠµ ì‹¤í–‰
  torchrun \
    --nproc_per_node=2 \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    src/training/train_distributed.py

  # ê²€ì¦ í•­ëª©:
  # - 2ê°œ í”„ë¡œì„¸ìŠ¤ ëª¨ë‘ ì‹œì‘
  # - Rank 0, 1 í• ë‹¹ í™•ì¸
  # - Gradient synchronization ë™ì‘
  # - MLflow ë¡œê¹… (main processë§Œ)
  # - ìµœì¢… ëª¨ë¸ ì¼ì¹˜ (all processes)
  ```

- [ ] **4-GPU DDP í…ŒìŠ¤íŠ¸ (p3.8xlarge í•„ìš”)**
  ```bash
  torchrun --nproc_per_node=4 src/training/train_distributed.py
  ```

- [ ] **Gradient ë™ê¸°í™” ê²€ì¦**
  - [ ] ê° GPUì—ì„œ ê³„ì‚°ëœ gradientê°€ ë™ì¼í•œì§€ í™•ì¸
  - [ ] All-reduce í›„ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¼ì¹˜ ê²€ì¦
  - [ ] Loss ìˆ˜ë ´ íŒ¨í„´ í™•ì¸

- [ ] **ì†ì‹¤ ìˆ˜ë ´ í™•ì¸**
  - [ ] Single GPU vs DDP ì†ì‹¤ ê³¡ì„  ë¹„êµ
  - [ ] ìµœì¢… ì •í™•ë„ ë¹„êµ (ë™ì¼í•´ì•¼ í•¨)

#### 2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

**ìš°ì„ ìˆœìœ„**: High
**ì˜ˆìƒ ì†Œìš”**: 2-4ì‹œê°„

- [ ] **í•™ìŠµ ì†ë„ ë¹„êµ**

  | ì„¤ì • | Throughput (images/sec) | í•™ìŠµ ì‹œê°„ (10 epochs) | GPU ë©”ëª¨ë¦¬ | Speedup |
  |------|------------------------|---------------------|-----------|---------|
  | 1-GPU | ??? | ??? | ??? | 1.0x |
  | 2-GPU DDP | ??? | ??? | ??? | ???x |
  | 4-GPU DDP | ??? | ??? | ??? | ???x |

  ```bash
  # ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸
  # scripts/benchmark_ddp.sh

  #!/bin/bash
  for nproc in 1 2 4; do
    echo "Testing with $nproc GPUs"

    if [ $nproc -eq 1 ]; then
      # Single GPU
      time poetry run python -m src.training.train
    else
      # DDP
      time torchrun --nproc_per_node=$nproc src/training/train_distributed.py
    fi

    # ë©”íŠ¸ë¦­ ê¸°ë¡
    # ...
  done
  ```

- [ ] **GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„**
  ```bash
  # nvidia-smië¡œ ëª¨ë‹ˆí„°ë§
  watch -n 1 nvidia-smi
  ```

- [ ] **í†µì‹  ì˜¤ë²„í—¤ë“œ ì¸¡ì •**
  - [ ] All-reduce ì‹œê°„ ì¸¡ì •
  - [ ] Gradient ë™ê¸°í™” ì˜¤ë²„í—¤ë“œ
  - [ ] ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì˜í–¥

- [ ] **ë°°ì¹˜ í¬ê¸° í™•ì¥ì„± í…ŒìŠ¤íŠ¸**
  - [ ] 1-GPU: batch_size=64
  - [ ] 2-GPU: batch_size=128 (2x)
  - [ ] 4-GPU: batch_size=256 (4x)
  - [ ] Effective batch size ì¦ê°€ì— ë”°ë¥¸ ì •í™•ë„ ë³€í™”

#### 3. ìµœì í™”

**ìš°ì„ ìˆœìœ„**: Medium
**ì˜ˆìƒ ì†Œìš”**: 4-8ì‹œê°„

- [ ] **Gradient Accumulation êµ¬í˜„**
  ```python
  # src/training/train.py

  accumulation_steps = 4
  optimizer.zero_grad()

  for i, (inputs, targets) in enumerate(train_loader):
      outputs = model(inputs)
      loss = criterion(outputs, targets) / accumulation_steps
      loss.backward()

      if (i + 1) % accumulation_steps == 0:
          optimizer.step()
          optimizer.zero_grad()
  ```

- [ ] **All-reduce ì „ëµ ìµœì í™”**
  - [ ] Bucket size ì¡°ì •
  - [ ] Gradient compression ì‹œë„
  - [ ] ë¹„ë™ê¸° all-reduce ì‹¤í—˜

- [ ] **Mixed Precision Training (AMP) í†µí•©**
  ```python
  from torch.cuda.amp import autocast, GradScaler

  scaler = GradScaler()

  for inputs, targets in train_loader:
      with autocast():
          outputs = model(inputs)
          loss = criterion(outputs, targets)

      scaler.scale(loss).backward()
      scaler.step(optimizer)
      scaler.update()
  ```

- [ ] **DataLoader ì„±ëŠ¥ íŠœë‹**
  - [ ] `num_workers` ìµœì í™” (CPU ì½”ì–´ ìˆ˜ ê³ ë ¤)
  - [ ] `pin_memory=True` íš¨ê³¼ ì¸¡ì •
  - [ ] `prefetch_factor` ì¡°ì •

- [ ] **ëª¨ë¸ ë³‘ë ¬í™” ì‹¤í—˜** (ë§¤ìš° í° ëª¨ë¸ìš©)
  - [ ] Pipeline parallelism
  - [ ] Tensor parallelism

#### 4. ë¬¸ì„œí™”

**ìš°ì„ ìˆœìœ„**: High
**ì˜ˆìƒ ì†Œìš”**: 2-3ì‹œê°„

- [ ] **ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì •ë¦¬**
  - [ ] `docs/ddp_benchmark_results.md` ìƒì„±
  - [ ] ì„±ëŠ¥ ê·¸ë˜í”„ ë° í‘œ
  - [ ] ë¶„ì„ ë° ê²°ë¡ 

- [ ] **í´ë¼ìš°ë“œ ì„¤ì • ê°€ì´ë“œ ì‘ì„±**
  - [ ] `docs/cloud_setup_guide.md` ìƒì„±
  - [ ] AWS/GCP ë‹¨ê³„ë³„ ì„¤ì • ë°©ë²•
  - [ ] ë¹„ìš© ìµœì í™” íŒ

- [ ] **DDP ë¬¸ì„œ ì—…ë°ì´íŠ¸**
  - [ ] `docs/distributed_training.md` ì—…ë°ì´íŠ¸
  - [ ] ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜
  - [ ] Best practices ì •ë¦¬

- [ ] **ë¹„ìš© ìµœì í™” ë¬¸ì„œí™”**
  - [ ] Spot Instance í™œìš©ë²•
  - [ ] ìë™ ì¢…ë£Œ ìŠ¤í¬ë¦½íŠ¸
  - [ ] ì˜ˆì‚° ì•Œë¦¼ ì„¤ì •

---

### ì˜ˆìƒ ë¹„ìš© (AWS ê¸°ì¤€)

#### Spot Instance ì‚¬ìš© ì‹œ (ì¶”ì²œ)

| ì‘ì—… | ì¸ìŠ¤í„´ìŠ¤ | GPU | ì˜ˆìƒ ì‹œê°„ | ì‹œê°„ë‹¹ ë¹„ìš© | ì´ ë¹„ìš© |
|------|----------|-----|----------|-----------|---------|
| í™˜ê²½ ì„¤ì • | p3.2xlarge | 1x V100 | 1ì‹œê°„ | $0.90 | $0.90 |
| 1-GPU ë² ì´ìŠ¤ë¼ì¸ | p3.2xlarge | 1x V100 | 1ì‹œê°„ | $0.90 | $0.90 |
| 2-GPU DDP í…ŒìŠ¤íŠ¸ | p3.8xlarge | 4x V100 | 2ì‹œê°„ | $3.60 | $7.20 |
| 4-GPU DDP í…ŒìŠ¤íŠ¸ | p3.8xlarge | 4x V100 | 2ì‹œê°„ | $3.60 | $7.20 |
| ìµœì í™” ì‹¤í—˜ | p3.8xlarge | 4x V100 | 4ì‹œê°„ | $3.60 | $14.40 |
| **ì´ê³„** | - | - | **10ì‹œê°„** | - | **$30.60** |

#### ë¹„ìš© ì ˆê° íŒ

1. **Spot Instance í™œìš©**: 70% ë¹„ìš© ì ˆê°
2. **ì‘ì—… ìë™ ì¢…ë£Œ**: ìœ íœ´ ì‹œê°„ ì œê±°
   ```bash
   # í•™ìŠµ ì™„ë£Œ í›„ ìë™ ì¢…ë£Œ
   poetry run python -m src.training.train && sudo shutdown -h now
   ```
3. **EBS ë³¼ë¥¨ ìµœì†Œí™”**: 100GB â†’ 50GB
4. **S3 ë°ì´í„° ìºì‹±**: ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë°˜ë³µ ë°©ì§€

---

### ìš°ì„ ìˆœìœ„ ë° ì¼ì •

**ìš°ì„ ìˆœìœ„**: **Medium**

Phase 3 ì™„ë£Œ í›„ ë˜ëŠ” ì‹¤ì œ í”„ë¡œë•ì…˜ ë°°í¬ ì „ì— ìˆ˜í–‰

**ì„ í–‰ ì¡°ê±´**:
- âœ… Phase 3.3 DDP ì½”ë“œ ì™„ì„±
- âœ… ë¡œì»¬ CPU DDP ê¸°ë³¸ ë™ì‘ ê²€ì¦

**ë¸”ë¡œì»¤**:
- â³ í´ë¼ìš°ë“œ í™˜ê²½ ë¯¸êµ¬ì¶•
- â³ ì˜ˆì‚° ìŠ¹ì¸ í•„ìš” (~$30-50)

**ì˜ˆìƒ ì¼ì •**:
- í´ë¼ìš°ë“œ í™˜ê²½ ì¤€ë¹„: 2025-11ì›” (Phase 3 ì™„ë£Œ í›„)
- DDP í…ŒìŠ¤íŠ¸ ìˆ˜í–‰: 2ì¼ (ì§‘ì¤‘ ì‘ì—…)
- ê²°ê³¼ ë¬¸ì„œí™”: 1ì¼

---

## ëª¨ë¸ ìµœì í™”

### Quantization (ì–‘ìí™”)

**ìš°ì„ ìˆœìœ„**: Medium
**ëª©ì **: ëª¨ë¸ í¬ê¸° ì¶•ì†Œ, ì¶”ë¡  ì†ë„ í–¥ìƒ

- [ ] **PyTorch Dynamic Quantization**
  ```python
  import torch.quantization as quantization

  model_fp32 = torch.load("model.pth")
  model_int8 = quantization.quantize_dynamic(
      model_fp32, {torch.nn.Linear}, dtype=torch.qint8
  )

  # í¬ê¸° ë¹„êµ
  # ì •í™•ë„ ë¹„êµ
  # ì¶”ë¡  ì†ë„ ë¹„êµ
  ```

- [ ] **Post-Training Static Quantization**
  - [ ] Calibration dataset ì¤€ë¹„
  - [ ] ì •í™•ë„ ì†ì‹¤ ì¸¡ì • (ëª©í‘œ: <1%)

- [ ] **Quantization-Aware Training (QAT)**
  - [ ] í•™ìŠµ ì¤‘ quantization ì‹œë®¬ë ˆì´ì…˜
  - [ ] ì •í™•ë„ í–¥ìƒ ê°€ëŠ¥ì„± íƒìƒ‰

**ëª©í‘œ**:
- ëª¨ë¸ í¬ê¸°: 10MB â†’ 2-3MB (INT8)
- ì¶”ë¡  ì†ë„: 2-3x í–¥ìƒ
- ì •í™•ë„ ì†ì‹¤: <1%

---

### Pruning (ê°€ì§€ì¹˜ê¸°)

**ìš°ì„ ìˆœìœ„**: Low
**ëª©ì **: ëª¨ë¸ ê²½ëŸ‰í™”, ì¶”ë¡  ì†ë„ í–¥ìƒ

- [ ] **ë¹„êµ¬ì¡°í™” Pruning (Unstructured)**
  ```python
  import torch.nn.utils.prune as prune

  # L1 unstructured pruning
  prune.l1_unstructured(model.conv1, name="weight", amount=0.3)
  ```

- [ ] **êµ¬ì¡°í™” Pruning (Structured)**
  - [ ] Channel pruning
  - [ ] Filter pruning

**ëª©í‘œ**:
- íŒŒë¼ë¯¸í„° ìˆ˜: 30% ê°ì†Œ
- ì •í™•ë„ ìœ ì§€: >89%

---

### Knowledge Distillation

**ìš°ì„ ìˆœìœ„**: Low
**ëª©ì **: ì‘ì€ ëª¨ë¸ë¡œ í° ëª¨ë¸ì˜ ì„±ëŠ¥ ì¬í˜„

- [ ] **Teacher ëª¨ë¸ í•™ìŠµ** (ResNet18)
- [ ] **Student ëª¨ë¸ ì„¤ê³„** (MobileNetV3-Small)
- [ ] **Distillation ì†ì‹¤ êµ¬í˜„**
  ```python
  # Soft targets from teacher
  # Hard targets from ground truth
  # Temperature scaling
  ```

**ëª©í‘œ**:
- Student ì •í™•ë„: Teacherì˜ 95% ìˆ˜ì¤€
- ëª¨ë¸ í¬ê¸°: 50% ì¶•ì†Œ
- ì¶”ë¡  ì†ë„: 2x í–¥ìƒ

---

### ONNX ë³€í™˜ ë° ìµœì í™”

**ìš°ì„ ìˆœìœ„**: Medium
**ëª©ì **: ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ì—ì„œ ëª¨ë¸ ì‚¬ìš©

- [ ] **PyTorch â†’ ONNX ë³€í™˜**
  ```python
  import torch.onnx

  dummy_input = torch.randn(1, 3, 32, 32)
  torch.onnx.export(
      model,
      dummy_input,
      "model.onnx",
      input_names=["image"],
      output_names=["logits"],
      dynamic_axes={"image": {0: "batch_size"}}
  )
  ```

- [ ] **ONNX Runtime ì¶”ë¡  í…ŒìŠ¤íŠ¸**
- [ ] **TensorRT ìµœì í™”** (GPU)
- [ ] **OpenVINO ìµœì í™”** (CPU)

---

## ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œì„ 

### Dataset Caching

**ìš°ì„ ìˆœìœ„**: Low
**ëª©ì **: ë°˜ë³µ í•™ìŠµ ì‹œ ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ

- [ ] **ë©”ëª¨ë¦¬ ìºì‹± (ì‘ì€ ë°ì´í„°ì…‹)**
  ```python
  class CachedDataset(Dataset):
      def __init__(self, dataset):
          self.cache = [dataset[i] for i in range(len(dataset))]

      def __getitem__(self, idx):
          return self.cache[idx]
  ```

- [ ] **ë””ìŠ¤í¬ ìºì‹± (í° ë°ì´í„°ì…‹)**
  - [ ] h5py or zarr ì‚¬ìš©

---

### Prefetching ìµœì í™”

**ìš°ì„ ìˆœìœ„**: Low

- [ ] **CUDA Stream í™œìš©**
- [ ] **DataLoader prefetch_factor íŠœë‹**
- [ ] **Multi-process data loading ìµœì í™”**

---

### ê³ ê¸‰ Augmentation

**ìš°ì„ ìˆœìœ„**: Medium
**ëª©ì **: ì •í™•ë„ í–¥ìƒ

- [ ] **AutoAugment êµ¬í˜„**
- [ ] **RandAugment êµ¬í˜„**
- [ ] **CutMix / MixUp**

---

## MLflow ê³ ê¸‰ ê¸°ëŠ¥

### MLflow Projects

**ìš°ì„ ìˆœìœ„**: Low
**ëª©ì **: ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ì‹¤í–‰

- [ ] **MLproject íŒŒì¼ ì‘ì„±**
  ```yaml
  name: vision-training

  conda_env: conda.yaml

  entry_points:
    main:
      parameters:
        epochs: {type: int, default: 20}
        batch_size: {type: int, default: 64}
      command: "python -m src.training.train"
  ```

- [ ] **MLflow Project ì‹¤í–‰ í…ŒìŠ¤íŠ¸**
  ```bash
  mlflow run . -P epochs=50
  ```

---

### MLflow Models Serving

**ìš°ì„ ìˆœìœ„**: Medium
**ëª©ì **: REST APIë¡œ ëª¨ë¸ ì„œë¹™

- [ ] **Model signature ì •ì˜**
  ```python
  from mlflow.models.signature import infer_signature

  signature = infer_signature(input_sample, output_sample)
  mlflow.pytorch.log_model(model, "model", signature=signature)
  ```

- [ ] **Local serving í…ŒìŠ¤íŠ¸**
  ```bash
  mlflow models serve -m "models:/vision-classifier/Production" -p 8080
  ```

- [ ] **FastAPI wrapper ì‘ì„±**
- [ ] **ì¶”ë¡  ìµœì í™”** (ë°°ì¹˜ ì²˜ë¦¬)

---

## CI/CD ê°œì„ 

### GPU Runner ì„¤ì •

**ìš°ì„ ìˆœìœ„**: Low
**ëª©ì **: GPU í…ŒìŠ¤íŠ¸ ìë™í™”

- [ ] **Self-hosted runner ì„¤ì •**
  - [ ] AWS EC2 GPU ì¸ìŠ¤í„´ìŠ¤
  - [ ] GitHub Actions self-hosted runner ë“±ë¡

- [ ] **GPU í…ŒìŠ¤íŠ¸ workflow ì¶”ê°€**
  ```yaml
  # .github/workflows/gpu-test.yml
  jobs:
    gpu-test:
      runs-on: [self-hosted, gpu]
      steps:
        - name: Run GPU tests
          run: poetry run pytest tests/test_gpu.py
  ```

---

### Nightly ì „ì²´ í…ŒìŠ¤íŠ¸

**ìš°ì„ ìˆœìœ„**: Low

- [ ] **ë§¤ì¼ ë°¤ slow í…ŒìŠ¤íŠ¸ í¬í•¨ ì‹¤í–‰**
  ```yaml
  on:
    schedule:
      - cron: '0 2 * * *'  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ (UTC)
  ```

---

### Performance Regression í…ŒìŠ¤íŠ¸

**ìš°ì„ ìˆœìœ„**: Low

- [ ] **ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥**
- [ ] **ì„±ëŠ¥ ì €í•˜ ê°ì§€**
- [ ] **ì•Œë¦¼ ì„¤ì •**

---

## ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### Prometheus Metrics

**ìš°ì„ ìˆœìœ„**: Medium (Phase 6-7)

- [ ] **í•™ìŠµ ë©”íŠ¸ë¦­ ë…¸ì¶œ**
  ```python
  from prometheus_client import Counter, Gauge

  training_loss = Gauge('training_loss', 'Current training loss')
  training_accuracy = Gauge('training_accuracy', 'Current training accuracy')
  ```

- [ ] **Prometheus ì„œë²„ ì„¤ì •**
- [ ] **Metrics endpoint** (`/metrics`)

---

### Grafana ëŒ€ì‹œë³´ë“œ

**ìš°ì„ ìˆœìœ„**: Medium (Phase 6-7)

- [ ] **Grafana ì„¤ì¹˜ ë° ì„¤ì •**
- [ ] **ëŒ€ì‹œë³´ë“œ êµ¬ì¶•**
  - [ ] í•™ìŠµ ì§„í–‰ ìƒí™©
  - [ ] GPU ì‚¬ìš©ë¥ 
  - [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
  - [ ] MLflow runs í†µê³„

---

### Alert ì„¤ì •

**ìš°ì„ ìˆœìœ„**: Low

- [ ] **í•™ìŠµ ì‹¤íŒ¨ ì•Œë¦¼** (Slack, Email)
- [ ] **ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼**
- [ ] **ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì•Œë¦¼**

---

## ì¸í”„ë¼ í™•ì¥

### Kubernetes ë°°í¬ (Phase 7)

**ìš°ì„ ìˆœìœ„**: Low (Phase 7)

- [ ] **Helm Chart ì‘ì„±**
- [ ] **MLflow server ë°°í¬**
- [ ] **PostgreSQL StatefulSet**
- [ ] **MinIO ë°°í¬**

---

### Airflow í†µí•© (Phase 8)

**ìš°ì„ ìˆœìœ„**: Low (Phase 8)

- [ ] **DAG ì‘ì„±** (ì¼ì¼ í•™ìŠµ íŒŒì´í”„ë¼ì¸)
- [ ] **DockerOperator ì‚¬ìš©**
- [ ] **ì‹¤íŒ¨ ì¬ì‹œë„ ë¡œì§**

---

## ì°¸ê³ 

ì´ TODO ëª©ë¡ì€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.

**ìš°ì„ ìˆœìœ„ ì •ì˜**:
- **High**: Phase 3 ì§í›„ ë˜ëŠ” í”„ë¡œë•ì…˜ ë°°í¬ ì „ í•„ìˆ˜
- **Medium**: ì„±ëŠ¥ ê°œì„  ë˜ëŠ” ê¸°ëŠ¥ í™•ì¥ ì‹œ í•„ìš”
- **Low**: ì„ íƒì , ì‹œê°„ ì—¬ìœ  ì‹œ ìˆ˜í–‰

**ê´€ë ¨ ë¬¸ì„œ**:
- [Phase 3 ìƒì„¸ ê³„íš](docs/phase3_plan.md)
- [ì „ì²´ ê³„íš](plan.md)
- [ë¶„ì‚° í•™ìŠµ ê°€ì´ë“œ](docs/distributed_training.md)

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-18
